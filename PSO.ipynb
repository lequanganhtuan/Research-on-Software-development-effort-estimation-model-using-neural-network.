{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc47e67",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91a7f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ch·∫°y pipeline cho dataset: telecom\n",
      "üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A2A12AAC00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A2973C67A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 2.8051\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 2.0869\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 2.0418\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 1.8857\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 1.7906\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 1.7257\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 1.6803\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'filters': 3, 'l2_reg': np.float64(0.0031127975917563788), 'dense_units': 14, 'dropout_rate': np.float64(0.20233249557643337), 'learning_rate': np.float64(0.002293869379966328), 'batch_size': 2, 'epochs': 62}\n",
      "üìâ Score t·ªët nh·∫•t: 1.6803\n",
      "\n",
      "üìÇ Fold 1/7\n",
      "‚úÖ Fold 1 RMSE: 191.8122\n",
      "\n",
      "üìÇ Fold 2/7\n",
      "‚úÖ Fold 2 RMSE: 237.5577\n",
      "\n",
      "üìÇ Fold 3/7\n",
      "‚úÖ Fold 3 RMSE: 27.0459\n",
      "\n",
      "üìÇ Fold 4/7\n",
      "‚úÖ Fold 4 RMSE: 201.6736\n",
      "\n",
      "üìÇ Fold 5/7\n",
      "‚úÖ Fold 5 RMSE: 146.5623\n",
      "\n",
      "üìÇ Fold 6/7\n",
      "‚úÖ Fold 6 RMSE: 125.3177\n",
      "\n",
      "üìÇ Fold 7/7\n",
      "‚úÖ Fold 7 RMSE: 137.8180\n",
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
      "üìå MAE    : 305.8111\n",
      "üìå MSE    : 111851.5161\n",
      "üìå RMSE   : 334.4421\n",
      "üìå R2     : -6.6280\n",
      "üìå MMRE   : 0.7317\n",
      "üìå MDMRE  : 0.7302\n",
      "üìå PRED25 : 0.0000\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/telecom_CNN_results.csv'\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/telecom_CNN_model.keras'\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/telecom_CNN_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# C·∫•u h√¨nh b·∫≠t/t·∫Øt c√°c b∆∞·ªõc\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': False,\n",
    "    'normalize_features': True,\n",
    "    'log_transform_target': True,\n",
    "    'save_model': True,\n",
    "    'save_results': True,\n",
    "    'visualize_results': True,\n",
    "    'noise_factor': 0.002,\n",
    "    'num_noise_copies': 1,\n",
    "    'test_size': 0.15,\n",
    "    'k_folds': 7,\n",
    "}\n",
    "\n",
    "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    if CONFIG['log_transform_target']:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true = np.expm1(y_true)\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# H√†m ƒë·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng n·∫øu b·∫≠t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªïi log cho m·ª•c ti√™u n·∫øu b·∫≠t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# H√†m x√¢y d·ª±ng m√¥ h√¨nh CNN\n",
    "def build_cnn_model(input_shape, filters=8, l2_reg=0.01, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(filters, kernel_size=1, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters, kernel_size=1, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(4, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Kh√¥ng gian si√™u tham s·ªë\n",
    "param_bounds = {\n",
    "    'filters': (4, 6),\n",
    "    'l2_reg': (0.01, 0.03),\n",
    "    'dense_units': (4, 8),\n",
    "    'dropout_rate': (0.3, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-3),\n",
    "    'batch_size': (4, 8),\n",
    "    'epochs': (20, 50)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['filters'][0], param_bounds['filters'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'filters': int(particle[0]),\n",
    "        'l2_reg': particle[1],\n",
    "        'dense_units': int(particle[2]),\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_cnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=5e-6)\n",
    "        \n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# H√†m ch·∫°y PSO\n",
    "def run_pso_cnn(X_train, y_train, num_particles=10, max_iter=12):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.7, 1.0, 2.0\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=5e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nüìÇ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"‚úÖ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # T√≠nh trung b√¨nh l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# H√†m tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung b√¨nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nüöÄ Ch·∫°y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    X_aug = X_aug.reshape(X_aug.shape[0], X_aug.shape[1], 1)\n",
    "    \n",
    "    # Chia t·∫≠p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y PSO\n",
    "    print(\"üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_cnn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "    print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"üìå {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"üìå {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_CNN_results.csv', index=False)\n",
    "        print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/{dataset_name}_CNN_results.csv'\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_CNN_model.keras')\n",
    "        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/{dataset_name}_CNN_model.keras'\")\n",
    "    \n",
    "    # Tr·ª±c quan h√≥a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/{dataset_name}_CNN_results.png'\")\n",
    "\n",
    "# C·∫•u h√¨nh c√°c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'telecom',\n",
    "        'path': 'telecom_processed.csv',\n",
    "        'feature_columns': [\n",
    "            'CHANGES', 'FILES'\n",
    "        ],\n",
    "        'target_column': 'ACT_EFF',\n",
    "        'numeric_columns': [\n",
    "            'CHANGES', 'FILES'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bab446",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17844f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ch·∫°y pipeline cho dataset: telecom\n",
      "üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 302.8631\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 292.2591\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 279.2649\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 278.7630\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 268.3132\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 248.3344\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 211.5361\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 210.7738\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 209.5651\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 168.8428\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 159.0536\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 157.0925\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'hidden_units1': 8, 'hidden_units2': 16, 'l2_reg': np.float64(0.019220520911461696), 'dropout_rate': np.float64(0.28891057270834125), 'learning_rate': np.float64(0.01), 'batch_size': 18, 'epochs': 100}\n",
      "üìâ Score t·ªët nh·∫•t: 157.0925\n",
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 371.7609\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 179.2214\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 367.7113\n",
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
      "üìå MAE    : 209.4971\n",
      "üìå MSE    : 69948.8572\n",
      "üìå RMSE   : 264.4785\n",
      "üìå R2     : -1.5513\n",
      "üìå MMRE   : 0.9818\n",
      "üìå MDMRE  : 0.9858\n",
      "üìå PRED25 : 0.0000\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/telecom_mlp_results.csv'\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/telecom_mlp_model.keras'\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/telecom_mlp_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# C·∫•u h√¨nh b·∫≠t/t·∫Øt c√°c b∆∞·ªõc\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # B·∫≠t/t·∫Øt tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "    'normalize_features': False,   # B·∫≠t/t·∫Øt chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
    "    'log_transform_target': False, # B·∫≠t/t·∫Øt bi·∫øn ƒë·ªïi log cho bi·∫øn m·ª•c ti√™u\n",
    "    'save_model': True,            # B·∫≠t/t·∫Øt l∆∞u m√¥ h√¨nh\n",
    "    'save_results': True,          # B·∫≠t/t·∫Øt l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "    'visualize_results': True,     # B·∫≠t/t·∫Øt tr·ª±c quan h√≥a\n",
    "    'noise_factor': 0.01,          # ƒê·ªô l·ªõn nhi·ªÖu Gaussian\n",
    "    'num_noise_copies': 2,         # S·ªë b·∫£n sao nhi·ªÖu\n",
    "    'test_size': 0.15,             # T·ª∑ l·ªá t·∫≠p test\n",
    "    'k_folds': 3,                  # S·ªë fold trong cross-validation\n",
    "}\n",
    "\n",
    "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# H√†m ƒë·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng n·∫øu b·∫≠t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªïi log cho m·ª•c ti√™u n·∫øu b·∫≠t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# H√†m x√¢y d·ª±ng m√¥ h√¨nh MLP\n",
    "def build_mlp_model(input_dim, hidden_units1=16, hidden_units2=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(hidden_units1, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(hidden_units2, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Kh√¥ng gian si√™u tham s·ªë cho MLP\n",
    "param_bounds = {\n",
    "    'hidden_units1': (8, 32),\n",
    "    'hidden_units2': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['hidden_units1'][0], param_bounds['hidden_units1'][1] + 1),\n",
    "        np.random.randint(param_bounds['hidden_units2'][0], param_bounds['hidden_units2'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'hidden_units1': int(particle[0]),\n",
    "        'hidden_units2': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_mlp_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# H√†m ch·∫°y PSO\n",
    "def run_pso_mlp(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nüìÇ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"‚úÖ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # T√≠nh trung b√¨nh l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# H√†m tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung b√¨nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_mlp_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nüöÄ Ch·∫°y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Chia t·∫≠p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y PSO\n",
    "    print(\"üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_mlp(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "    print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"üìå {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"üìå {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_mlp_results.csv', index=False)\n",
    "        print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/{dataset_name}_mlp_results.csv'\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_mlp_model.keras')\n",
    "        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/{dataset_name}_mlp_model.keras'\")\n",
    "    \n",
    "    # Tr·ª±c quan h√≥a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/{dataset_name}_mlp_results.png'\")\n",
    "\n",
    "# C·∫•u h√¨nh c√°c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'telecom',\n",
    "        'path': 'telecom_processed.csv',\n",
    "        'feature_columns': [\n",
    "            'CHANGES', 'FILES'\n",
    "        ],\n",
    "        'target_column': 'ACT_EFF',\n",
    "        'numeric_columns': [\n",
    "            'CHANGES', 'FILES'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ch·∫°y pipeline cho t·ª´ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851a234",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ch·∫°y pipeline cho dataset: desharnais\n",
      "üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.2331\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.2070\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.2040\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1954\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1946\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1709\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'lstm_units': 17, 'dense_units': 10, 'l2_reg': np.float64(0.05290511907499294), 'dropout_rate': np.float64(0.2388252925394604), 'learning_rate': np.float64(0.01028603584658312), 'batch_size': 13, 'epochs': 105}\n",
      "üìâ Score t·ªët nh·∫•t: 0.1709\n",
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.1741\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.1602\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.1102\n",
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
      "üìå MAE    : 0.1038\n",
      "üìå MSE    : 0.0185\n",
      "üìå RMSE   : 0.1361\n",
      "üìå R2     : 0.9869\n",
      "üìå MMRE   : 3.4108\n",
      "üìå MDMRE  : 0.0612\n",
      "üìå PRED25 : 88.2353\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/desharnais_lstm_results.csv'\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/desharnais_lstm_model.keras'\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/desharnais_lstm_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# C·∫•u h√¨nh b·∫≠t/t·∫Øt c√°c b∆∞·ªõc\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # B·∫≠t/t·∫Øt tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "    'normalize_features': False,   # B·∫≠t/t·∫Øt chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
    "    'log_transform_target': False, # B·∫≠t/t·∫Øt bi·∫øn ƒë·ªïi log cho bi·∫øn m·ª•c ti√™u\n",
    "    'save_model': True,            # B·∫≠t/t·∫Øt l∆∞u m√¥ h√¨nh\n",
    "    'save_results': True,          # B·∫≠t/t·∫Øt l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "    'visualize_results': True,     # B·∫≠t/t·∫Øt tr·ª±c quan h√≥a\n",
    "    'noise_factor': 0.01,          # ƒê·ªô l·ªõn nhi·ªÖu Gaussian\n",
    "    'num_noise_copies': 2,         # S·ªë b·∫£n sao nhi·ªÖu\n",
    "    'test_size': 0.15,             # T·ª∑ l·ªá t·∫≠p test\n",
    "    'k_folds': 3,                  # S·ªë fold trong cross-validation\n",
    "    'timesteps': 1                 # S·ªë b∆∞·ªõc th·ªùi gian cho LSTM\n",
    "}\n",
    "\n",
    "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# H√†m ƒë·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng n·∫øu b·∫≠t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªïi log cho m·ª•c ti√™u n·∫øu b·∫≠t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# H√†m reshape d·ªØ li·ªáu cho LSTM\n",
    "def reshape_for_lstm(X, timesteps=1):\n",
    "    # Reshape d·ªØ li·ªáu th√†nh [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# H√†m x√¢y d·ª±ng m√¥ h√¨nh LSTM\n",
    "def build_lstm_model(input_shape, lstm_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(lstm_units, activation='tanh', recurrent_activation='sigmoid', return_sequences=False,\n",
    "             kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Kh√¥ng gian si√™u tham s·ªë cho LSTM\n",
    "param_bounds = {\n",
    "    'lstm_units': (8, 32),\n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# H√†m ch·∫°y PSO\n",
    "def run_pso_lstm(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nüìÇ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"‚úÖ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # T√≠nh trung b√¨nh l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# H√†m tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung b√¨nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_lstm_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nüöÄ Ch·∫°y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape d·ªØ li·ªáu cho LSTM\n",
    "    X_aug = reshape_for_lstm(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]\n",
    "    \n",
    "    # Chia t·∫≠p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y PSO\n",
    "    print(\"üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_lstm(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "    print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"üìå {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"üìå {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_lstm_results.csv', index=False)\n",
    "        print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/{dataset_name}_lstm_results.csv'\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_lstm_model.keras')\n",
    "        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/{dataset_name}_lstm_model.keras'\")\n",
    "    \n",
    "    # Tr·ª±c quan h√≥a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/{dataset_name}_lstm_results.png'\")\n",
    "\n",
    "# C·∫•u h√¨nh c√°c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'telecom_processed.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ch·∫°y pipeline cho t·ª´ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a647c",
   "metadata": {},
   "source": [
    "# RBFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248a319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ch·∫°y pipeline cho dataset: telecom\n",
      "üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 276.4030\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 275.7365\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 266.2788\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 266.0440\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 264.0681\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 261.8590\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 257.8452\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'num_centers': 20, 'sigma': 1.5086470855348315, 'learning_rate': 0.011201058720142022, 'batch_size': 13, 'epochs': 102}\n",
      "üìâ Score t·ªët nh·∫•t: 257.8452\n",
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 376.9539\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 124.3902\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 245.7883\n",
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
      "üìå MAE    : 96.4265\n",
      "üìå MSE    : 26049.8507\n",
      "üìå RMSE   : 161.3997\n",
      "üìå R2     : 0.0499\n",
      "üìå MMRE   : 0.3701\n",
      "üìå MDMRE  : 0.1491\n",
      "üìå PRED25 : 55.5556\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/telecom_rbfn_results.csv'\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/telecom_rbfn_model.keras'\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/telecom_rbfn_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# C·∫•u h√¨nh b·∫≠t/t·∫Øt c√°c b∆∞·ªõc\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # B·∫≠t/t·∫Øt tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "    'normalize_features': False,   # B·∫≠t/t·∫Øt chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
    "    'log_transform_target': False, # B·∫≠t/t·∫Øt bi·∫øn ƒë·ªïi log cho bi·∫øn m·ª•c ti√™u\n",
    "    'save_model': True,            # B·∫≠t/t·∫Øt l∆∞u m√¥ h√¨nh\n",
    "    'save_results': True,          # B·∫≠t/t·∫Øt l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "    'visualize_results': True,     # B·∫≠t/t·∫Øt tr·ª±c quan h√≥a\n",
    "    'noise_factor': 0.01,          # ƒê·ªô l·ªõn nhi·ªÖu Gaussian\n",
    "    'num_noise_copies': 2,         # S·ªë b·∫£n sao nhi·ªÖu\n",
    "    'test_size': 0.15,             # T·ª∑ l·ªá t·∫≠p test\n",
    "    'k_folds': 3,                  # S·ªë fold trong cross-validation\n",
    "}\n",
    "\n",
    "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# H√†m ƒë·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng n·∫øu b·∫≠t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªïi log cho m·ª•c ti√™u n·∫øu b·∫≠t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# T·∫ßng RBF t√πy ch·ªânh\n",
    "class RBFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, sigma, centers, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.sigma = float(sigma)\n",
    "        self.centers = tf.Variable(centers, trainable=False, dtype=tf.float32, name='centers')\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.rbf_weights = self.add_weight(\n",
    "            name='rbf_weights',\n",
    "            shape=(self.units, 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.rbf_biases = self.add_weight(\n",
    "            name='rbf_biases',\n",
    "            shape=(1,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        diff = tf.expand_dims(inputs, axis=1) - tf.expand_dims(self.centers, axis=0)\n",
    "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "        output = tf.exp(-l2 / (2.0 * self.sigma ** 2))\n",
    "        return tf.matmul(output, self.rbf_weights) + self.rbf_biases\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(RBFLayer, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'sigma': float(self.sigma),\n",
    "            'centers': self.centers.numpy().tolist()\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# H√†m x√¢y d·ª±ng m√¥ h√¨nh RBFN\n",
    "def build_rbfn_model(input_dim, num_centers=10, sigma=1.0, learning_rate=0.001, X_train=None):\n",
    "    if X_train is None:\n",
    "        raise ValueError(\"X_train must be provided to initialize centers with KMeans\")\n",
    "    \n",
    "    # Kh·ªüi t·∫°o centers b·∫±ng KMeans\n",
    "    n_clusters = max(int(num_centers), 1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    centers = np.array(kmeans.cluster_centers_, dtype=np.float32)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        RBFLayer(units=int(num_centers), sigma=sigma, centers=centers, name='rbf_layer'),\n",
    "        Dense(1, activation='linear', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Kh√¥ng gian si√™u tham s·ªë cho RBFN\n",
    "param_bounds = {\n",
    "    'num_centers': (5, 20),\n",
    "    'sigma': (0.1, 2.0),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['num_centers'][0], param_bounds['num_centers'][1] + 1),\n",
    "        np.random.uniform(param_bounds['sigma'][0], param_bounds['sigma'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'num_centers': int(particle[0]),\n",
    "        'sigma': float(particle[1]),\n",
    "        'learning_rate': float(particle[2]),\n",
    "        'batch_size': int(particle[3]),\n",
    "        'epochs': int(particle[4])\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rbfn_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        X_train=X_train,\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# H√†m ch·∫°y PSO\n",
    "def run_pso_rbfn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['sigma'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_rbfn_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        X_train=X_train,\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nüìÇ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"‚úÖ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # T√≠nh trung b√¨nh l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# H√†m tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung b√¨nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_rbfn_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nüöÄ Ch·∫°y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Chia t·∫≠p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y PSO\n",
    "    print(\"üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_rbfn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "    print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"üìå {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"üìå {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_rbfn_results.csv', index=False)\n",
    "        print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/{dataset_name}_rbfn_results.csv'\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_rbfn_model.keras')\n",
    "        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/{dataset_name}_rbfn_model.keras'\")\n",
    "    \n",
    "    # Tr·ª±c quan h√≥a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/{dataset_name}_rbfn_results.png'\")\n",
    "\n",
    "# C·∫•u h√¨nh c√°c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'telecom',\n",
    "        'path': 'telecom_processed.csv',\n",
    "        'feature_columns': [\n",
    "            'CHANGES', 'FILES'\n",
    "        ],\n",
    "        'target_column': 'ACT_EFF',\n",
    "        'numeric_columns': [\n",
    "            'CHANGES', 'FILES'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ch·∫°y pipeline cho t·ª´ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18597b1b",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbde1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ch·∫°y pipeline cho dataset: desharnais\n",
      "üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1618\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1569\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1564\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1502\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1476\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1470\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'rnn_units': 19, 'dense_units': 6, 'l2_reg': np.float64(0.04578883850996157), 'dropout_rate': np.float64(0.1755520424929638), 'learning_rate': np.float64(0.009449979979976523), 'batch_size': 13, 'epochs': 104}\n",
      "üìâ Score t·ªët nh·∫•t: 0.1470\n",
      "\n",
      "üìå Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.1408\n",
      "\n",
      "üìå Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.1700\n",
      "\n",
      "üìå Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.1077\n",
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
      "üìå MAE    : 0.0879\n",
      "üìå MSE    : 0.0145\n",
      "üìå RMSE   : 0.1205\n",
      "üìå R2     : 0.9897\n",
      "üìå MMRE   : 3.2531\n",
      "üìå MDMRE  : 0.0423\n",
      "üìå PRED25 : 88.2353\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/desharnais_rnn_results.csv'\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/desharnais_rnn_model.keras'\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/desharnais_rnn_results.png'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# C·∫•u h√¨nh b·∫≠t/t·∫Øt c√°c b∆∞·ªõc\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # B·∫≠t/t·∫Øt tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "    'normalize_features': False,   # B·∫≠t/t·∫Øt chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
    "    'log_transform_target': False, # B·∫≠t/t·∫Øt bi·∫øn ƒë·ªïi log cho bi·∫øn m·ª•c ti√™u\n",
    "    'save_model': True,            # B·∫≠t/t·∫Øt l∆∞u m√¥ h√¨nh\n",
    "    'save_results': True,          # B·∫≠t/t·∫Øt l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "    'visualize_results': True,     # B·∫≠t/t·∫Øt tr·ª±c quan h√≥a\n",
    "    'noise_factor': 0.01,          # ƒê·ªô l·ªõn nhi·ªÖu Gaussian\n",
    "    'num_noise_copies': 2,         # S·ªë b·∫£n sao nhi·ªÖu\n",
    "    'test_size': 0.15,             # T·ª∑ l·ªá t·∫≠p test\n",
    "    'k_folds': 3,                  # S·ªë fold trong cross-validation\n",
    "    'timesteps': 1                 # S·ªë b∆∞·ªõc th·ªùi gian cho RNN \n",
    "}\n",
    "\n",
    "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# H√†m ƒë·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng n·∫øu b·∫≠t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªïi log cho m·ª•c ti√™u n·∫øu b·∫≠t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# H√†m reshape d·ªØ li·ªáu cho RNN\n",
    "def reshape_for_rnn(X, timesteps=1):\n",
    "    # Reshape d·ªØ li·ªáu th√†nh [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# H√†m x√¢y d·ª±ng m√¥ h√¨nh RNN\n",
    "def build_rnn_model(input_shape, rnn_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        SimpleRNN(rnn_units, activation='tanh', return_sequences=False, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Kh√¥ng gian si√™u tham s·ªë cho RNN\n",
    "param_bounds = {\n",
    "    'rnn_units': (8, 32),\n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['rnn_units'][0], param_bounds['rnn_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'rnn_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# H√†m ch·∫°y PSO\n",
    "def run_pso_rnn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand() * np.ones(dim)\n",
    "            r2 = np.random.uniform(0, 1, dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, -1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "                \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_rnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nüìå Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"‚úÖ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # T√≠nh trung b√¨nh l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# H√†m tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung b√¨nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_rnn_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nüöÄ Ch·∫°y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape d·ªØ li·ªáu cho RNN\n",
    "    X_aug = reshape_for_rnn(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]  # C·∫Øt y ƒë·ªÉ kh·ªõp s·ªë m·∫´u sau reshape\n",
    "    \n",
    "    # Chia t·∫≠p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y PSO\n",
    "    print(\"üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_rnn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "    print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"üìå {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"üìå {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_rnn_results.csv', index=False)\n",
    "        print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/{dataset_name}_rnn_results.csv'\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_rnn_model.keras')\n",
    "        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/{dataset_name}_rnn_model.keras'\")\n",
    "    \n",
    "    # Tr·ª±c quan h√≥a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/{dataset_name}_rnn_results.png'\")\n",
    "\n",
    "# C·∫•u h√¨nh c√°c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'telecom_processed.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ch·∫°y pipeline cho t·ª´ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b6f04",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ch·∫°y pipeline cho dataset: desharnais\n",
      "üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1341\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1324\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'lstm_units': 10, 'dense_units': 1, 'l2_reg': np.float64(0.018236164417873006), 'dropout_rate': np.float64(0.32471572716955666), 'learning_rate': np.float64(0.005911359310043052), 'batch_size': 13, 'epochs': 112}\n",
      "üìâ Score t·ªët nh·∫•t: 0.1324\n",
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.3708\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.3822\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.2721\n",
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
      "üìå MAE    : 0.3196\n",
      "üìå MSE    : 0.1606\n",
      "üìå RMSE   : 0.4008\n",
      "üìå R2     : 0.8867\n",
      "üìå MMRE   : 2.4827\n",
      "üìå MDMRE  : 0.3384\n",
      "üìå PRED25 : 29.4118\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/desharnais_bilstm_results.csv'\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/desharnais_bilstm_model.keras'\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/desharnais_bilstm_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# C·∫•u h√¨nh b·∫≠t/t·∫Øt c√°c b∆∞·ªõc\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # B·∫≠t/t·∫Øt tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "    'normalize_features': False,   # B·∫≠t/t·∫Øt chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
    "    'log_transform_target': False, # B·∫≠t/t·∫Øt bi·∫øn ƒë·ªïi log cho bi·∫øn m·ª•c ti√™u\n",
    "    'save_model': True,            # B·∫≠t/t·∫Øt l∆∞u m√¥ h√¨nh\n",
    "    'save_results': True,          # B·∫≠t/t·∫Øt l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "    'visualize_results': True,     # B·∫≠t/t·∫Øt tr·ª±c quan h√≥a\n",
    "    'noise_factor': 0.01,          # ƒê·ªô l·ªõn nhi·ªÖu Gaussian\n",
    "    'num_noise_copies': 2,         # S·ªë b·∫£n sao nhi·ªÖu\n",
    "    'test_size': 0.15,             # T·ª∑ l·ªá t·∫≠p test\n",
    "    'k_folds': 3,                  # S·ªë fold trong cross-validation\n",
    "    'timesteps': 1                 # S·ªë b∆∞·ªõc th·ªùi gian cho BiLSTM \n",
    "}\n",
    "\n",
    "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# H√†m ƒë·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng n·∫øu b·∫≠t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªïi log cho m·ª•c ti√™u n·∫øu b·∫≠t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng nhi·ªÖu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# H√†m reshape d·ªØ li·ªáu cho BiLSTM\n",
    "def reshape_for_bilstm(X, timesteps=1):\n",
    "    # Reshape d·ªØ li·ªáu th√†nh [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# H√†m x√¢y d·ª±ng m√¥ h√¨nh BiLSTM\n",
    "def build_bilstm_model(input_shape, lstm_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=False, kernel_regularizer=l2(l2_reg))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Kh√¥ng gian si√™u tham s·ªë cho BiLSTM\n",
    "param_bounds = {\n",
    "    'lstm_units': (8, 16),  \n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (4, 16), \n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_bilstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# H√†m ch·∫°y PSO\n",
    "def run_pso_bilstm(X_train, y_train, num_particles=10, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh t·ªëi ∆∞u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_bilstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nüìÇ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"‚úÖ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # T√≠nh trung b√¨nh l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# H√†m tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung b√¨nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_bilstm_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nüöÄ Ch·∫°y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape d·ªØ li·ªáu cho BiLSTM\n",
    "    X_aug = reshape_for_bilstm(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]  # C·∫Øt y ƒë·ªÉ kh·ªõp s·ªë m·∫´u sau reshape\n",
    "    \n",
    "    # Chia t·∫≠p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y PSO\n",
    "    print(\"üîç T√¨m si√™u tham s·ªë t·ªëi ∆∞u b·∫±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_bilstm(X_train, y_train, num_particles=10, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "    print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"üìå {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"üìå {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_bilstm_results.csv', index=False)\n",
    "        print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o 'results/{dataset_name}_bilstm_results.csv'\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_bilstm_model.keras')\n",
    "        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'models/{dataset_name}_bilstm_model.keras'\")\n",
    "    \n",
    "    # Tr·ª±c quan h√≥a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'visualizations/{dataset_name}_bilstm_results.png'\")\n",
    "\n",
    "# C·∫•u h√¨nh c√°c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'telecom_processed.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ch·∫°y pipeline cho t·ª´ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
