{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a99598",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c2a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f170e6e",
   "metadata": {},
   "source": [
    "## Define the evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911df5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac7a49",
   "metadata": {},
   "source": [
    "## Read file and declare columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4db5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "\n",
    "\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Effort', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fe23e",
   "metadata": {},
   "source": [
    "## Create feature set and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201afac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numeric_columns + binary_columns\n",
    "X = df[features].values\n",
    "y = df['Effort'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e5951",
   "metadata": {},
   "source": [
    "## Data enhancement with Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f2ff3",
   "metadata": {},
   "source": [
    "Data augmentation by adding light Gaussian noise to the input data X aims to: \n",
    "\n",
    "- Improve the model's generalization, \n",
    "\n",
    "- Combat overfitting (memorizing the training data), \n",
    "\n",
    "- Increase data diversity without needing to collect more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937873fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation with Gaussian noise ===\n",
      "X_augmented shape: (243, 20)\n",
      "y_augmented shape: (243,)\n"
     ]
    }
   ],
   "source": [
    "def add_gaussian_noise(X, noise_factor=0.01):\n",
    "    if not np.issubdtype(X.dtype, np.number):\n",
    "        raise ValueError(\"Input X must be numeric.\")\n",
    "    if np.any(np.isnan(X)):\n",
    "        raise ValueError(\"Input X contains NaN values.\")\n",
    "    noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "    return X + noise\n",
    "\n",
    "X_augmented = X.copy()\n",
    "y_augmented = y.copy()\n",
    "for _ in range(2):  \n",
    "    X_noisy = add_gaussian_noise(X, noise_factor=0.01)\n",
    "    X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "    y_augmented = np.hstack((y_augmented, y))\n",
    "\n",
    "print(\"\\n=== After data augmentation with Gaussian noise ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9668f0c",
   "metadata": {},
   "source": [
    "## Reshape and Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5325075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data size after reshape ===\n",
      "X_augmented shape: (243, 20, 1)\n",
      "y_augmented shape: (243,)\n",
      "\n",
      "✅ Data size of CNN:\n",
      " - X_train: (206, 20, 1)\n",
      " - X_test : (37, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_augmented = X_augmented.reshape(X_augmented.shape[0], X_augmented.shape[1], 1)\n",
    "\n",
    "print(\"\\n=== Data size after reshape ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n✅ Data size of CNN:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f80255",
   "metadata": {},
   "source": [
    "- CNN requires at least 3 dimensions of input:\n",
    "With tabular or vector data, you need to add a channel dimension for CNN to understand that it is a spatial dimension or image channel\n",
    "\n",
    "- Use an 85% training and 15% testing ratio with random_state=42 for replication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a5ccf",
   "metadata": {},
   "source": [
    "## Building the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32dd063",
   "metadata": {},
   "source": [
    "Define the function to build a CNN model with Conv1D, BatchNormalization, Flatten, Dense, and Dropout layers.\n",
    "\n",
    "- The model uses Conv1D layers with the ReLU activation function, batch normalization, and Dropout to avoid overfitting. \n",
    "- It uses the Huber loss function and is optimized with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7082d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(filters=8, l2_reg=0.01, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b062bfd",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4e194",
   "metadata": {},
   "source": [
    "Define the hyperparameter space and the supporting functions for the Particle Swarm Optimization (PSO) algorithm.\n",
    "\n",
    "- Define the limits for hyperparameters (filters, l2_reg, dense_units, dropout_rate, learning_rate, batch_size, epochs). \n",
    "- The **random_particle** function creates a random particle. \n",
    "- The **decode_particle** function converts the particle into model parameters. \n",
    "- The **fitness_function** evaluates the model's performance using the average RMSE through K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02af338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'filters': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dense_units': (8, 32),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['filters'][0], param_bounds['filters'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'filters': int(particle[0]),\n",
    "        'l2_reg': particle[1],\n",
    "        'dense_units': int(particle[2]),\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    params['l2_reg'] = min(params['l2_reg'], param_bounds['l2_reg'][1])\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_cnn_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9af68a",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772613c",
   "metadata": {},
   "source": [
    "Using the PSO algorithm to find the optimal hyperparameter set. \n",
    "- Initialize num_particles=15 and max_iter=10. \n",
    "\n",
    "- Update the position and velocity of the particles based on the PSO formula. \n",
    "\n",
    "- Find the best particle (g_best_position) and the best score (g_best_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b6aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Chạy PSO để tìm siêu tham số tối ưu...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.1535\n",
      "✅ Cập nhật g_best: Score = 0.1367\n",
      "\n",
      "🔁 Iteration 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Chạy PSO\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Chạy PSO để tìm siêu tham số tối ưu...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m best_particle, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pso_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m best_params \u001b[38;5;241m=\u001b[39m decode_particle(best_particle)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏆 Siêu tham số tốt nhất: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m, in \u001b[0;36mrun_pso_cnn\u001b[1;34m(num_particles, max_iter)\u001b[0m\n\u001b[0;32m     32\u001b[0m particles[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(particles[i][\u001b[38;5;241m1\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_reg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     33\u001b[0m particles[i][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i][\u001b[38;5;241m3\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 35\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m p_best_scores[i]:\n\u001b[0;32m     38\u001b[0m     p_best_scores[i] \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(particle)\u001b[0m\n\u001b[0;32m     49\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     55\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    367\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:104\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator())\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_pso_cnn(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['l2_reg'][0])\n",
    "            particles[i][1] = min(particles[i][1], param_bounds['l2_reg'][1])\n",
    "            particles[i][3] = np.clip(particles[i][3], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Chạy PSO\n",
    "print(\"🚀 Chạy PSO để tìm siêu tham số tối ưu...\")\n",
    "best_particle, best_score = run_pso_cnn(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "print(f\"📉 Score tốt nhất: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcd04c",
   "metadata": {},
   "source": [
    "## Training the optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d22b5",
   "metadata": {},
   "source": [
    "Use the best hyperparameters to train the CNN model with K-Fold cross-validation. \n",
    "Details: \n",
    "- Use K-Fold with 3 folds. \n",
    "- Train the model with EarlyStopping and ReduceLROnPlateau to optimize. \n",
    "- Calculate the average RMSE across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76829c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.1435\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.1316\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.0786\n",
      "\n",
      "📊 RMSE trung bình qua 3 folds: 0.1179\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_cnn_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"✅ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Lưu lịch sử huấn luyện\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\n📊 RMSE trung bình qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8eef9",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c10519",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set and calculate metrics such as MSE, RMSE, MAE, R², MAPE, MMRE, MdMRE, PRED(25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ca85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\n",
      "📌 MSE     : 0.0527 ± 0.0262\n",
      "📌 RMSE    : 0.2222 ± 0.0576\n",
      "📌 MAE     : 0.1272 ± 0.0295\n",
      "📌 R²      : 0.9623 ± 0.0169\n",
      "📌 MAPE    : 56.05% ± 32.00%\n",
      "📌 MMRE    : 0.5605 ± 0.3200\n",
      "📌 MdMRE   : 0.1296 ± 0.0960\n",
      "📌 PRED(25): 64.25% ± 11.67%\n",
      "\n",
      "Đã lưu kết quả đánh giá vào 'cnn_evaluation_results_scaled.csv'\n",
      "\n",
      "Đã lưu hình ảnh trực quan hóa vào 'cnn_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Tính các chỉ số đánh giá\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# Đánh giá bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In kết quả\n",
    "print(\"\\n📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\")\n",
    "print(f\"📌 MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"📌 RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"📌 MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"📌 R²      : {np.mean(bootstrap_metrics['r2']):.4f} ± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"📌 MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% ± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"📌 MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} ± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"📌 MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} ± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"📌 PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% ± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# Lưu kết quả đánh giá\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('cnn_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nĐã lưu kết quả đánh giá vào 'cnn_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trực quan hóa kết quả\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung bình qua các folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nĐã lưu hình ảnh trực quan hóa vào 'cnn_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f3a21",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945a8c1",
   "metadata": {},
   "source": [
    "## Import libraries and set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286836dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de345bb",
   "metadata": {},
   "source": [
    "## Define the evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136485f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32ab0d",
   "metadata": {},
   "source": [
    "## Define custom Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293ed6c",
   "metadata": {},
   "source": [
    "Define 4 custom Transformer layers for data preprocessing: \n",
    "- **DataTypeConverter**: Convert data types of numeric and binary columns. \n",
    "- **NaNImputer**: Fill missing values (NaN) with the mean of the numeric column. \n",
    "- **SelectiveScaler**: Standardize (standard scaling) numeric columns. \n",
    "- **DataAugmenterLSTM**: Augment data with Gaussian noise and reshape data into the format 3D **(samples, timesteps, features)** for LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols, binary_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.binary_cols = binary_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce')\n",
    "        for col in self.binary_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce').astype('int')\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để xử lý NaN\n",
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.means_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numeric_cols:\n",
    "            self.means_[col] = X[col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col].fillna(self.means_[col], inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để tiêu chuẩn hóa (chỉ các cột số)\n",
    "class SelectiveScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler_ = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.numeric_cols] = self.scaler_.transform(X_copy[self.numeric_cols])\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để tăng cường dữ liệu và reshape cho LSTM\n",
    "class DataAugmenterLSTM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, noise_factor=0.01, n_copies=2, timesteps=1):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.n_copies = n_copies\n",
    "        self.timesteps = timesteps\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_augmented = X.copy()\n",
    "        y_augmented = y.copy() if y is not None else None\n",
    "        for _ in range(self.n_copies):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_factor, size=X.shape)\n",
    "            X_noisy = X + noise\n",
    "            X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "            if y is not None:\n",
    "                y_augmented = np.hstack((y_augmented, y))\n",
    "        # Reshape cho LSTM: (samples, timesteps, features)\n",
    "        X_augmented = X_augmented.reshape(X_augmented.shape[0], self.timesteps, X_augmented.shape[1])\n",
    "        return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d9bbb",
   "metadata": {},
   "source": [
    "## Read and check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a51fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "features = numeric_columns + binary_columns\n",
    "target = 'Effort'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a9ae2",
   "metadata": {},
   "source": [
    "## Create and apply a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca20e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lequa\\AppData\\Local\\Temp\\ipykernel_11776\\2711339975.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(self.means_[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('data_type_converter', DataTypeConverter(numeric_cols=numeric_columns, binary_cols=binary_columns)),\n",
    "    ('nan_imputer', NaNImputer(numeric_cols=numeric_columns)),\n",
    "    ('scaler', SelectiveScaler(numeric_cols=numeric_columns)),\n",
    "])\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7176be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation with Gaussian noise ===\n",
      "X_augmented shape: (243, 1, 19)\n",
      "y_augmented shape: (243,)\n"
     ]
    }
   ],
   "source": [
    "data_augmenter = DataAugmenterLSTM(noise_factor=0.01, n_copies=2, timesteps=1)\n",
    "X_augmented, y_augmented = data_augmenter.transform(X_transformed.values, y)\n",
    "\n",
    "print(\"\\n=== After data augmentation with Gaussian noise ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b8313",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22979563",
   "metadata": {},
   "source": [
    "Split the augmented data (X_augmented, y_augmented) into a training set (85%) and a test set (15%) using train_test_split with random_state=42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580db9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data size of LSTM:\n",
      " - X_train: (206, 1, 19)\n",
      " - X_test : (37, 1, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n✅ Data size of LSTM:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c0d42",
   "metadata": {},
   "source": [
    "## Building LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efeb88",
   "metadata": {},
   "source": [
    "Define the function build_lstm_model to create an LSTM model with the architecture: \n",
    "- **Input** layer with shape (timesteps, features). \n",
    "- **LSTM** layer with the number of units (lstm_units) and does not return sequences (return_sequences=False). \n",
    "- D**ropou**t layer to reduce overfitting. \n",
    "- **Dense** layer with dense_units and relu activation. \n",
    "- Second **Dropout** layer. \n",
    "- Final **Dense** layer with 1 unit and linear activation to predict Effort.\n",
    "\n",
    "Use the **Huber** loss function and optimize with **Adam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(lstm_units=32, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92594e",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38a020",
   "metadata": {},
   "source": [
    "Define the hyperparameter space (param_bounds) for lstm_units, dense_units, dropout_rate, learning_rate, batch_size, epochs. \n",
    "\n",
    "Use two functions: \n",
    "- **random_particle**: Encode hyperparameters into a random vector (particle). \n",
    "- **decode_particle**: Decode the vector into a hyperparameter dictionary.\n",
    "\n",
    "The fitness_function uses KFold cross-validation (3 folds) to assess the performance of each hyperparameter set based on RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'lstm_units': (16, 64),\n",
    "    'dense_units': (8, 32),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'dropout_rate': particle[2],\n",
    "        'learning_rate': particle[3],\n",
    "        'batch_size': int(particle[4]),\n",
    "        'epochs': int(particle[5])\n",
    "    }\n",
    "    params['dropout_rate'] = np.clip(params['dropout_rate'], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "    return params\n",
    "\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_lstm_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a84627",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca9029",
   "metadata": {},
   "source": [
    "Using the PSO algorithm to find the optimal hyperparameter set. \n",
    "- Initialize num_particles=15 and max_iter=10. \n",
    "\n",
    "- Update the position and velocity of the particles based on the PSO formula. \n",
    "\n",
    "- Find the best particle (g_best_position) and the best score (g_best_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Chạy PSO để tìm siêu tham số tối ưu...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "✅ Cập nhật g_best: Score = 0.0663\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'lstm_units': 57, 'dense_units': 20, 'dropout_rate': np.float64(0.2), 'learning_rate': np.float64(0.008488723679811452), 'batch_size': 28, 'epochs': 55}\n",
      "📉 Score tốt nhất: 0.0663\n"
     ]
    }
   ],
   "source": [
    "def run_pso_lstm(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = np.clip(particles[i][2], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Chạy PSO\n",
    "print(\"🚀 Chạy PSO để tìm siêu tham số tối ưu...\")\n",
    "best_particle, best_score = run_pso_lstm(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "print(f\"📉 Score tốt nhất: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eef3aa",
   "metadata": {},
   "source": [
    "## Optimal model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41229d",
   "metadata": {},
   "source": [
    "- Use the best hyperparameters from PSO to train the LSTM model on the entire training set with KFold cross-validation (3 folds). \n",
    "- Use EarlyStopping and ReduceLROnPlateau to avoid overfitting and adjust the learning rate. \n",
    "- Save the training history (loss, val_loss) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.0763\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.0844\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.0578\n",
      "\n",
      "📊 RMSE trung bình qua 3 folds: 0.0728\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_lstm_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"✅ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Lưu lịch sử huấn luyện\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\n📊 RMSE trung bình qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d016bca",
   "metadata": {},
   "source": [
    "## Model evaluation and bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\n",
      "📌 MSE     : 0.0086 ± 0.0029\n",
      "📌 RMSE    : 0.0916 ± 0.0156\n",
      "📌 MAE     : 0.0653 ± 0.0107\n",
      "📌 R²      : 0.9937 ± 0.0021\n",
      "📌 MAPE    : 32.12% ± 23.39%\n",
      "📌 MMRE    : 0.3212 ± 0.2339\n",
      "📌 MdMRE   : 0.0394 ± 0.0165\n",
      "📌 PRED(25): 88.12% ± 7.85%\n",
      "\n",
      "Đã lưu kết quả đánh giá vào 'lstm_evaluation_results_scaled.csv'\n",
      "\n",
      "Đã lưu hình ảnh trực quan hóa vào 'lstm_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Tính các chỉ số đánh giá\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# Đánh giá bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In kết quả\n",
    "print(\"\\n📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\")\n",
    "print(f\"📌 MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"📌 RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"📌 MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"📌 R²      : {np.mean(bootstrap_metrics['r2']):.4f} ± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"📌 MAPE    : {np.nanmean(bootstrap_metrics['mape']):.2f}% ± {np.nanstd(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"📌 MMRE    : {np.nanmean(bootstrap_metrics['mmre']):.4f} ± {np.nanstd(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"📌 MdMRE   : {np.nanmean(bootstrap_metrics['mdmre']):.4f} ± {np.nanstd(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"📌 PRED(25): {np.nanmean(bootstrap_metrics['pred25']):.2f}% ± {np.nanstd(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# Lưu kết quả đánh giá\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.nanmean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.nanstd(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.nanmean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.nanstd(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.nanmean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.nanstd(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.nanmean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.nanstd(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('lstm_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nĐã lưu kết quả đánh giá vào 'lstm_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trực quan hóa kết quả\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung bình qua các folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nĐã lưu hình ảnh trực quan hóa vào 'lstm_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72345498",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd7a9d",
   "metadata": {},
   "source": [
    "## Import libraries and set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6017ea6",
   "metadata": {},
   "source": [
    "## Define the evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2443a",
   "metadata": {},
   "source": [
    "## Define custom Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6428123",
   "metadata": {},
   "source": [
    "MLP only enhances the data with Gaussian noise without reshaping. This means that the data stays in 2D form (samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21060513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.means_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numeric_cols:\n",
    "            self.means_[col] = X[col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col].fillna(self.means_[col], inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để tiêu chuẩn hóa (chỉ các cột số)\n",
    "class SelectiveScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler_ = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.numeric_cols] = self.scaler_.transform(X_copy[self.numeric_cols])\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để tăng cường dữ liệu\n",
    "class DataAugmenter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, noise_factor=0.01, n_copies=2):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.n_copies = n_copies\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_augmented = X.copy()\n",
    "        y_augmented = y.copy() if y is not None else None\n",
    "        for _ in range(self.n_copies):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_factor, size=X.shape)\n",
    "            X_noisy = X + noise\n",
    "            X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "            if y is not None:\n",
    "                y_augmented = np.hstack((y_augmented, y))\n",
    "        return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea75f2b",
   "metadata": {},
   "source": [
    "## Read and check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6cf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "features = numeric_columns + binary_columns\n",
    "target = 'Effort'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d74f93",
   "metadata": {},
   "source": [
    "## Create and apply a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a3d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation with Gaussian noise ===\n",
      "X_augmented shape: (243, 19)\n",
      "y_augmented shape: (243,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lequa\\AppData\\Local\\Temp\\ipykernel_11776\\1988351987.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(self.means_[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('data_type_converter', DataTypeConverter(numeric_cols=numeric_columns, binary_cols=binary_columns)),\n",
    "    ('nan_imputer', NaNImputer(numeric_cols=numeric_columns)),\n",
    "    ('scaler', SelectiveScaler(numeric_cols=numeric_columns)),\n",
    "])\n",
    "\n",
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "data_augmenter = DataAugmenter(noise_factor=0.01, n_copies=2)\n",
    "X_augmented, y_augmented = data_augmenter.transform(X_transformed.values, y)\n",
    "\n",
    "print(\"\\n=== After data augmentation with Gaussian noise ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fccbf37",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data size of MLP:\n",
      " - X_train: (206, 19)\n",
      " - X_test : (37, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n✅ Data size of MLP:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618591c8",
   "metadata": {},
   "source": [
    "## Building MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f27aa",
   "metadata": {},
   "source": [
    "Define the build_mlp_model function to create an MLP model with the following architecture:\n",
    "\n",
    "- Input layer with shape (features,), i.e. 2D data.\n",
    "- First Dense layer with dense_units_1 unit and relu activation.\n",
    "- Dropout layer to reduce overfitting.\n",
    "- Second Dense layer with dense_units_2 units and relu activation.\n",
    "- Second Dropout layer.\n",
    "-  Dense layer with 1 unit and linear activation to predict Effort.\n",
    "\n",
    "Use Huber loss function and optimize with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(dense_units_1=64, dense_units_2=32, dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(dense_units_1, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units_2, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d2fbc",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d42986",
   "metadata": {},
   "source": [
    "Define hyperparameter space (param_bounds) for dense_units_1, dense_units_2, dropout_rate, learning_rate, batch_size, epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'dense_units_1': (32, 128),\n",
    "    'dense_units_2': (16, 64),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['dense_units_1'][0], param_bounds['dense_units_1'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units_2'][0], param_bounds['dense_units_2'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'dense_units_1': int(particle[0]),\n",
    "        'dense_units_2': int(particle[1]),\n",
    "        'dropout_rate': particle[2],\n",
    "        'learning_rate': particle[3],\n",
    "        'batch_size': int(particle[4]),\n",
    "        'epochs': int(particle[5])\n",
    "    }\n",
    "    params['dropout_rate'] = np.clip(params['dropout_rate'], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_mlp_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ecbb9",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Chạy PSO để tìm siêu tham số tối ưu...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.1050\n",
      "✅ Cập nhật g_best: Score = 0.0999\n",
      "✅ Cập nhật g_best: Score = 0.0952\n",
      "✅ Cập nhật g_best: Score = 0.0921\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "✅ Cập nhật g_best: Score = 0.0832\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.0772\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "✅ Cập nhật g_best: Score = 0.0765\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "✅ Cập nhật g_best: Score = 0.0761\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "✅ Cập nhật g_best: Score = 0.0724\n",
      "✅ Cập nhật g_best: Score = 0.0704\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'dense_units_1': 132, 'dense_units_2': 71, 'dropout_rate': np.float64(0.2), 'learning_rate': np.float64(0.0072998326507956734), 'batch_size': 28, 'epochs': 100}\n",
      "📉 Score tốt nhất: 0.0704\n"
     ]
    }
   ],
   "source": [
    "def run_pso_mlp(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = np.clip(particles[i][2], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Chạy PSO\n",
    "print(\"🚀 Chạy PSO để tìm siêu tham số tối ưu...\")\n",
    "best_particle, best_score = run_pso_mlp(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "print(f\"📉 Score tốt nhất: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d722f",
   "metadata": {},
   "source": [
    "## Optimal model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.0870\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.0782\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.0647\n",
      "\n",
      "📊 RMSE trung bình qua 3 folds: 0.0766\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_mlp_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"✅ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Lưu lịch sử huấn luyện\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\n📊 RMSE trung bình qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c50b7",
   "metadata": {},
   "source": [
    "## Model evaluation and bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdaf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\n",
      "📌 MSE     : 0.0116 ± 0.0041\n",
      "📌 RMSE    : 0.1062 ± 0.0190\n",
      "📌 MAE     : 0.0778 ± 0.0117\n",
      "📌 R²      : 0.9915 ± 0.0027\n",
      "📌 MAPE    : 82.88% ± 67.53%\n",
      "📌 MMRE    : 0.8288 ± 0.6753\n",
      "📌 MdMRE   : 0.0710 ± 0.0376\n",
      "📌 PRED(25): 82.21% ± 9.36%\n",
      "\n",
      "Đã lưu kết quả đánh giá vào 'mlp_evaluation_results_scaled.csv'\n",
      "\n",
      "Đã lưu hình ảnh trực quan hóa vào 'mlp_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Tính các chỉ số đánh giá\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# Đánh giá bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In kết quả\n",
    "print(\"\\n📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\")\n",
    "print(f\"📌 MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"📌 RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"📌 MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"📌 R²      : {np.mean(bootstrap_metrics['r2']):.4f} ± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"📌 MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% ± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"📌 MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} ± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"📌 MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} ± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"📌 PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% ± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# Lưu kết quả đánh giá\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('mlp_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nĐã lưu kết quả đánh giá vào 'mlp_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trực quan hóa kết quả\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung bình qua các folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mlp_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nĐã lưu hình ảnh trực quan hóa vào 'mlp_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96eff4a",
   "metadata": {},
   "source": [
    "# RBFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf086644",
   "metadata": {},
   "source": [
    "## Define the evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604879de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917ad2c",
   "metadata": {},
   "source": [
    "## Define custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3b092",
   "metadata": {},
   "source": [
    "RBFN only enhances the data with Gaussian noise without reshaping. This means that the data is kept in 2D form (samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d134b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols, binary_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.binary_cols = binary_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce')\n",
    "        for col in self.binary_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce').astype('int')\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để xử lý NaN\n",
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.means_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numeric_cols:\n",
    "            self.means_[col] = X[col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col].fillna(self.means_[col], inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để tiêu chuẩn hóa (chỉ các cột số)\n",
    "class SelectiveScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler_ = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.numeric_cols] = self.scaler_.transform(X_copy[self.numeric_cols])\n",
    "        return X_copy\n",
    "\n",
    "# Transformer tùy chỉnh để tăng cường dữ liệu\n",
    "class DataAugmenter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, noise_factor=0.01, n_copies=2):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.n_copies = n_copies\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_augmented = X.copy()\n",
    "        y_augmented = y.copy() if y is not None else None\n",
    "        for _ in range(self.n_copies):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_factor, size=X.shape)\n",
    "            X_noisy = X + noise\n",
    "            X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "            if y is not None:\n",
    "                y_augmented = np.hstack((y_augmented, y))\n",
    "        return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be274aec",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab20b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "features = numeric_columns + binary_columns\n",
    "target = 'Effort'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36c512",
   "metadata": {},
   "source": [
    "## Create and apply a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0896a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation ===\n",
      "X_augmented shape: (243, 19)\n",
      "y_augmented shape: (243,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lequa\\AppData\\Local\\Temp\\ipykernel_5832\\3040840636.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(self.means_[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('data_type_converter', DataTypeConverter(numeric_cols=numeric_columns, binary_cols=binary_columns)),\n",
    "    ('nan_imputer', NaNImputer(numeric_cols=numeric_columns)),\n",
    "    ('scaler', SelectiveScaler(numeric_cols=numeric_columns)),\n",
    "])\n",
    "\n",
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "data_augmenter = DataAugmenter(noise_factor=0.01, n_copies=2)\n",
    "X_augmented, y_augmented = data_augmenter.transform(X_transformed.values, y)\n",
    "\n",
    "print(\"\\n=== After data augmentation ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc695ed",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d018e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Kích thước dữ liệu RBFN:\n",
      " - X_train: (206, 19)\n",
      " - X_test : (37, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n✅ Kích thước dữ liệu RBFN:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed48b5",
   "metadata": {},
   "source": [
    "## Building RBFN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dff73",
   "metadata": {},
   "source": [
    "Define the build_rbfn_model function to create an RBFN model with the following architecture:\n",
    "\n",
    "- Input layer with shape (features,), i.e. 2D data.\n",
    "- Custom RBFLayer layer with n_centers centers and sigma parameter, implementing radial (Gaussian) basis function.\n",
    "- Final Dense layer with 1 unit and linear activation to predict Effort.\n",
    "\n",
    "Using Huber loss function and optimizing with Adam.\n",
    "\n",
    "\n",
    "RBFN: Uses the RBFLayer class to implement radial (Gaussian) basis functions, based on the Euclidean distance between the input and the centers. This class is specific to RBFN and does not appear in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3cce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFLayer(Layer):\n",
    "    def __init__(self, n_centers, sigma=1.0, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.n_centers = n_centers\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                      shape=(self.n_centers, input_shape[1]),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        self.sigma = self.add_weight(name='sigma',\n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.sigma),\n",
    "                                    trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Tính khoảng cách Euclidean bình phương\n",
    "        diff = tf.expand_dims(inputs, axis=1) - tf.expand_dims(self.centers, axis=0)\n",
    "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "        # Hàm Gaussian\n",
    "        return tf.exp(-l2 / (2.0 * tf.square(self.sigma)))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_centers)\n",
    "\n",
    "def build_rbfn_model(n_centers=20, sigma=1.0, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        RBFLayer(n_centers, sigma=sigma),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a1402",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb8674",
   "metadata": {},
   "source": [
    "Use n_centers (number of centers in RBFLayer) and sigma (parameter that adjusts the width of the Gaussian function), which are RBFN-specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "750597d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'n_centers': (10, 50),\n",
    "    'sigma': (0.1, 2.0),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['n_centers'][0], param_bounds['n_centers'][1] + 1),\n",
    "        np.random.uniform(param_bounds['sigma'][0], param_bounds['sigma'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'n_centers': int(particle[0]),\n",
    "        'sigma': particle[1],\n",
    "        'learning_rate': particle[2],\n",
    "        'batch_size': int(particle[3]),\n",
    "        'epochs': int(particle[4])\n",
    "    }\n",
    "    params['sigma'] = np.clip(params['sigma'], param_bounds['sigma'][0], param_bounds['sigma'][1])\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rbfn_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9962273",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc920c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Chạy PSO để tìm siêu tham số tối ưu...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "✅ Cập nhật g_best: Score = 0.1346\n",
      "✅ Cập nhật g_best: Score = 0.1313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Chạy PSO\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Chạy PSO để tìm siêu tham số tối ưu...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m best_particle, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pso_rbfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m best_params \u001b[38;5;241m=\u001b[39m decode_particle(best_particle)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏆 Siêu tham số tốt nhất: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mrun_pso_rbfn\u001b[1;34m(num_particles, max_iter)\u001b[0m\n\u001b[0;32m     30\u001b[0m particles[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i], bounds_array[:, \u001b[38;5;241m0\u001b[39m], bounds_array[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     31\u001b[0m particles[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i][\u001b[38;5;241m1\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m p_best_scores[i]:\n\u001b[0;32m     36\u001b[0m     p_best_scores[i] \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(particle)\u001b[0m\n\u001b[0;32m     42\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    367\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:112\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_pso_rbfn(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = np.clip(particles[i][1], param_bounds['sigma'][0], param_bounds['sigma'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Chạy PSO\n",
    "print(\"🚀 Chạy PSO để tìm siêu tham số tối ưu...\")\n",
    "best_particle, best_score = run_pso_rbfn(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "print(f\"📉 Score tốt nhất: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d4166",
   "metadata": {},
   "source": [
    "## Optimal model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf6051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.1113\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.1488\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.0943\n",
      "\n",
      "📊 RMSE trung bình qua 3 folds: 0.1181\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_rbfn_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"✅ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Lưu lịch sử huấn luyện\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\n📊 RMSE trung bình qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714002a3",
   "metadata": {},
   "source": [
    "## Model evaluation and bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d5e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\n",
      "📌 MSE     : 0.0116 ± 0.0040\n",
      "📌 RMSE    : 0.1060 ± 0.0187\n",
      "📌 MAE     : 0.0777 ± 0.0118\n",
      "📌 R²      : 0.9916 ± 0.0026\n",
      "📌 MAPE    : 82.30% ± 67.88%\n",
      "📌 MMRE    : 0.8230 ± 0.6788\n",
      "📌 MdMRE   : 0.0715 ± 0.0363\n",
      "📌 PRED(25): 82.35% ± 9.36%\n",
      "\n",
      "Đã lưu kết quả đánh giá vào 'rbfn_evaluation_results_scaled.csv'\n",
      "\n",
      "Đã lưu hình ảnh trực quan hóa vào 'rbfn_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Tính các chỉ số đánh giá\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# Đánh giá bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In kết quả\n",
    "print(\"\\n📈 Kết quả đánh giá bootstrap (trên giá trị đã scale):\")\n",
    "print(f\"📌 MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"📌 RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"📌 MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"📌 R²      : {np.mean(bootstrap_metrics['r2']):.4f} ± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"📌 MAPE    : {np.nanmean(bootstrap_metrics['mape']):.2f}% ± {np.nanstd(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"📌 MMRE    : {np.nanmean(bootstrap_metrics['mmre']):.4f} ± {np.nanstd(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"📌 MdMRE   : {np.nanmean(bootstrap_metrics['mdmre']):.4f} ± {np.nanstd(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"📌 PRED(25): {np.nanmean(bootstrap_metrics['pred25']):.2f}% ± {np.nanstd(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# Lưu kết quả đánh giá\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.nanmean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.nanstd(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.nanmean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.nanstd(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.nanmean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.nanstd(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.nanmean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.nanstd(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('rbfn_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nĐã lưu kết quả đánh giá vào 'rbfn_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trực quan hóa kết quả\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung bình qua các folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rbfn_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nĐã lưu hình ảnh trực quan hóa vào 'rbfn_visualization_results_scaled.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
