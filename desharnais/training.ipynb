{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a99598",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c2a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f170e6e",
   "metadata": {},
   "source": [
    "## Define the evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911df5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac7a49",
   "metadata": {},
   "source": [
    "## Read file and declare columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4db5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "\n",
    "\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Effort', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fe23e",
   "metadata": {},
   "source": [
    "## Create feature set and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201afac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numeric_columns + binary_columns\n",
    "X = df[features].values\n",
    "y = df['Effort'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e5951",
   "metadata": {},
   "source": [
    "## Data enhancement with Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f2ff3",
   "metadata": {},
   "source": [
    "Data augmentation by adding light Gaussian noise to the input data X aims to: \n",
    "\n",
    "- Improve the model's generalization, \n",
    "\n",
    "- Combat overfitting (memorizing the training data), \n",
    "\n",
    "- Increase data diversity without needing to collect more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937873fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation with Gaussian noise ===\n",
      "X_augmented shape: (243, 20)\n",
      "y_augmented shape: (243,)\n"
     ]
    }
   ],
   "source": [
    "def add_gaussian_noise(X, noise_factor=0.01):\n",
    "    if not np.issubdtype(X.dtype, np.number):\n",
    "        raise ValueError(\"Input X must be numeric.\")\n",
    "    if np.any(np.isnan(X)):\n",
    "        raise ValueError(\"Input X contains NaN values.\")\n",
    "    noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "    return X + noise\n",
    "\n",
    "X_augmented = X.copy()\n",
    "y_augmented = y.copy()\n",
    "for _ in range(2):  \n",
    "    X_noisy = add_gaussian_noise(X, noise_factor=0.01)\n",
    "    X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "    y_augmented = np.hstack((y_augmented, y))\n",
    "\n",
    "print(\"\\n=== After data augmentation with Gaussian noise ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9668f0c",
   "metadata": {},
   "source": [
    "## Reshape and Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5325075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data size after reshape ===\n",
      "X_augmented shape: (243, 20, 1)\n",
      "y_augmented shape: (243,)\n",
      "\n",
      "‚úÖ Data size of CNN:\n",
      " - X_train: (206, 20, 1)\n",
      " - X_test : (37, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_augmented = X_augmented.reshape(X_augmented.shape[0], X_augmented.shape[1], 1)\n",
    "\n",
    "print(\"\\n=== Data size after reshape ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n‚úÖ Data size of CNN:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f80255",
   "metadata": {},
   "source": [
    "- CNN requires at least 3 dimensions of input:\n",
    "With tabular or vector data, you need to add a channel dimension for CNN to understand that it is a spatial dimension or image channel\n",
    "\n",
    "- Use an 85% training and 15% testing ratio with random_state=42 for replication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a5ccf",
   "metadata": {},
   "source": [
    "## Building the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32dd063",
   "metadata": {},
   "source": [
    "Define the function to build a CNN model with Conv1D, BatchNormalization, Flatten, Dense, and Dropout layers.\n",
    "\n",
    "- The model uses Conv1D layers with the ReLU activation function, batch normalization, and Dropout to avoid overfitting. \n",
    "- It uses the Huber loss function and is optimized with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7082d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(filters=8, l2_reg=0.01, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b062bfd",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4e194",
   "metadata": {},
   "source": [
    "Define the hyperparameter space and the supporting functions for the Particle Swarm Optimization (PSO) algorithm.\n",
    "\n",
    "- Define the limits for hyperparameters (filters, l2_reg, dense_units, dropout_rate, learning_rate, batch_size, epochs). \n",
    "- The **random_particle** function creates a random particle. \n",
    "- The **decode_particle** function converts the particle into model parameters. \n",
    "- The **fitness_function** evaluates the model's performance using the average RMSE through K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02af338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'filters': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dense_units': (8, 32),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['filters'][0], param_bounds['filters'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'filters': int(particle[0]),\n",
    "        'l2_reg': particle[1],\n",
    "        'dense_units': int(particle[2]),\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    params['l2_reg'] = min(params['l2_reg'], param_bounds['l2_reg'][1])\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_cnn_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9af68a",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772613c",
   "metadata": {},
   "source": [
    "Using the PSO algorithm to find the optimal hyperparameter set. \n",
    "- Initialize num_particles=15 and max_iter=10. \n",
    "\n",
    "- Update the position and velocity of the particles based on the PSO formula. \n",
    "\n",
    "- Find the best particle (g_best_position) and the best score (g_best_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b6aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1535\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1367\n",
      "\n",
      "üîÅ Iteration 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Ch·∫°y PSO\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m best_particle, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pso_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m best_params \u001b[38;5;241m=\u001b[39m decode_particle(best_particle)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müèÜ Si√™u tham s·ªë t·ªët nh·∫•t: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m, in \u001b[0;36mrun_pso_cnn\u001b[1;34m(num_particles, max_iter)\u001b[0m\n\u001b[0;32m     32\u001b[0m particles[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(particles[i][\u001b[38;5;241m1\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_reg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     33\u001b[0m particles[i][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i][\u001b[38;5;241m3\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 35\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m p_best_scores[i]:\n\u001b[0;32m     38\u001b[0m     p_best_scores[i] \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(particle)\u001b[0m\n\u001b[0;32m     49\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     55\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    367\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:104\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator())\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_pso_cnn(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['l2_reg'][0])\n",
    "            particles[i][1] = min(particles[i][1], param_bounds['l2_reg'][1])\n",
    "            particles[i][3] = np.clip(particles[i][3], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Ch·∫°y PSO\n",
    "print(\"üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\")\n",
    "best_particle, best_score = run_pso_cnn(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcd04c",
   "metadata": {},
   "source": [
    "## Training the optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d22b5",
   "metadata": {},
   "source": [
    "Use the best hyperparameters to train the CNN model with K-Fold cross-validation. \n",
    "Details: \n",
    "- Use K-Fold with 3 folds. \n",
    "- Train the model with EarlyStopping and ReduceLROnPlateau to optimize. \n",
    "- Calculate the average RMSE across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76829c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.1435\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.1316\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.0786\n",
      "\n",
      "üìä RMSE trung b√¨nh qua 3 folds: 0.1179\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_cnn_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nüìÇ Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"‚úÖ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\nüìä RMSE trung b√¨nh qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8eef9",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c10519",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set and calculate metrics such as MSE, RMSE, MAE, R¬≤, MAPE, MMRE, MdMRE, PRED(25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ca85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\n",
      "üìå MSE     : 0.0527 ¬± 0.0262\n",
      "üìå RMSE    : 0.2222 ¬± 0.0576\n",
      "üìå MAE     : 0.1272 ¬± 0.0295\n",
      "üìå R¬≤      : 0.9623 ¬± 0.0169\n",
      "üìå MAPE    : 56.05% ¬± 32.00%\n",
      "üìå MMRE    : 0.5605 ¬± 0.3200\n",
      "üìå MdMRE   : 0.1296 ¬± 0.0960\n",
      "üìå PRED(25): 64.25% ¬± 11.67%\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'cnn_evaluation_results_scaled.csv'\n",
      "\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'cnn_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ƒê√°nh gi√° bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\")\n",
    "print(f\"üìå MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ¬± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"üìå RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ¬± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"üìå MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ¬± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"üìå R¬≤      : {np.mean(bootstrap_metrics['r2']):.4f} ¬± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"üìå MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% ¬± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"üìå MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} ¬± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"üìå MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} ¬± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"üìå PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% ¬± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('cnn_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'cnn_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung b√¨nh qua c√°c folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'cnn_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f3a21",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945a8c1",
   "metadata": {},
   "source": [
    "## Import libraries and set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286836dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de345bb",
   "metadata": {},
   "source": [
    "## Define the evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136485f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32ab0d",
   "metadata": {},
   "source": [
    "## Define custom Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293ed6c",
   "metadata": {},
   "source": [
    "Define 4 custom Transformer layers for data preprocessing: \n",
    "- **DataTypeConverter**: Convert data types of numeric and binary columns. \n",
    "- **NaNImputer**: Fill missing values (NaN) with the mean of the numeric column. \n",
    "- **SelectiveScaler**: Standardize (standard scaling) numeric columns. \n",
    "- **DataAugmenterLSTM**: Augment data with Gaussian noise and reshape data into the format 3D **(samples, timesteps, features)** for LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols, binary_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.binary_cols = binary_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce')\n",
    "        for col in self.binary_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce').astype('int')\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ x·ª≠ l√Ω NaN\n",
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.means_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numeric_cols:\n",
    "            self.means_[col] = X[col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col].fillna(self.means_[col], inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ ti√™u chu·∫©n h√≥a (ch·ªâ c√°c c·ªôt s·ªë)\n",
    "class SelectiveScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler_ = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.numeric_cols] = self.scaler_.transform(X_copy[self.numeric_cols])\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ tƒÉng c∆∞·ªùng d·ªØ li·ªáu v√† reshape cho LSTM\n",
    "class DataAugmenterLSTM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, noise_factor=0.01, n_copies=2, timesteps=1):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.n_copies = n_copies\n",
    "        self.timesteps = timesteps\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_augmented = X.copy()\n",
    "        y_augmented = y.copy() if y is not None else None\n",
    "        for _ in range(self.n_copies):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_factor, size=X.shape)\n",
    "            X_noisy = X + noise\n",
    "            X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "            if y is not None:\n",
    "                y_augmented = np.hstack((y_augmented, y))\n",
    "        # Reshape cho LSTM: (samples, timesteps, features)\n",
    "        X_augmented = X_augmented.reshape(X_augmented.shape[0], self.timesteps, X_augmented.shape[1])\n",
    "        return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d9bbb",
   "metadata": {},
   "source": [
    "## Read and check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a51fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "features = numeric_columns + binary_columns\n",
    "target = 'Effort'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a9ae2",
   "metadata": {},
   "source": [
    "## Create and apply a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca20e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lequa\\AppData\\Local\\Temp\\ipykernel_11776\\2711339975.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(self.means_[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('data_type_converter', DataTypeConverter(numeric_cols=numeric_columns, binary_cols=binary_columns)),\n",
    "    ('nan_imputer', NaNImputer(numeric_cols=numeric_columns)),\n",
    "    ('scaler', SelectiveScaler(numeric_cols=numeric_columns)),\n",
    "])\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7176be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation with Gaussian noise ===\n",
      "X_augmented shape: (243, 1, 19)\n",
      "y_augmented shape: (243,)\n"
     ]
    }
   ],
   "source": [
    "data_augmenter = DataAugmenterLSTM(noise_factor=0.01, n_copies=2, timesteps=1)\n",
    "X_augmented, y_augmented = data_augmenter.transform(X_transformed.values, y)\n",
    "\n",
    "print(\"\\n=== After data augmentation with Gaussian noise ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b8313",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22979563",
   "metadata": {},
   "source": [
    "Split the augmented data (X_augmented, y_augmented) into a training set (85%) and a test set (15%) using train_test_split with random_state=42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580db9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data size of LSTM:\n",
      " - X_train: (206, 1, 19)\n",
      " - X_test : (37, 1, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n‚úÖ Data size of LSTM:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c0d42",
   "metadata": {},
   "source": [
    "## Building LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efeb88",
   "metadata": {},
   "source": [
    "Define the function build_lstm_model to create an LSTM model with the architecture: \n",
    "- **Input** layer with shape (timesteps, features). \n",
    "- **LSTM** layer with the number of units (lstm_units) and does not return sequences (return_sequences=False). \n",
    "- D**ropou**t layer to reduce overfitting. \n",
    "- **Dense** layer with dense_units and relu activation. \n",
    "- Second **Dropout** layer. \n",
    "- Final **Dense** layer with 1 unit and linear activation to predict Effort.\n",
    "\n",
    "Use the **Huber** loss function and optimize with **Adam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(lstm_units=32, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92594e",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38a020",
   "metadata": {},
   "source": [
    "Define the hyperparameter space (param_bounds) for lstm_units, dense_units, dropout_rate, learning_rate, batch_size, epochs. \n",
    "\n",
    "Use two functions: \n",
    "- **random_particle**: Encode hyperparameters into a random vector (particle). \n",
    "- **decode_particle**: Decode the vector into a hyperparameter dictionary.\n",
    "\n",
    "The fitness_function uses KFold cross-validation (3 folds) to assess the performance of each hyperparameter set based on RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'lstm_units': (16, 64),\n",
    "    'dense_units': (8, 32),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'dropout_rate': particle[2],\n",
    "        'learning_rate': particle[3],\n",
    "        'batch_size': int(particle[4]),\n",
    "        'epochs': int(particle[5])\n",
    "    }\n",
    "    params['dropout_rate'] = np.clip(params['dropout_rate'], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "    return params\n",
    "\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_lstm_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a84627",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca9029",
   "metadata": {},
   "source": [
    "Using the PSO algorithm to find the optimal hyperparameter set. \n",
    "- Initialize num_particles=15 and max_iter=10. \n",
    "\n",
    "- Update the position and velocity of the particles based on the PSO formula. \n",
    "\n",
    "- Find the best particle (g_best_position) and the best score (g_best_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0663\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'lstm_units': 57, 'dense_units': 20, 'dropout_rate': np.float64(0.2), 'learning_rate': np.float64(0.008488723679811452), 'batch_size': 28, 'epochs': 55}\n",
      "üìâ Score t·ªët nh·∫•t: 0.0663\n"
     ]
    }
   ],
   "source": [
    "def run_pso_lstm(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = np.clip(particles[i][2], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Ch·∫°y PSO\n",
    "print(\"üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\")\n",
    "best_particle, best_score = run_pso_lstm(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eef3aa",
   "metadata": {},
   "source": [
    "## Optimal model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41229d",
   "metadata": {},
   "source": [
    "- Use the best hyperparameters from PSO to train the LSTM model on the entire training set with KFold cross-validation (3 folds). \n",
    "- Use EarlyStopping and ReduceLROnPlateau to avoid overfitting and adjust the learning rate. \n",
    "- Save the training history (loss, val_loss) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.0763\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.0844\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.0578\n",
      "\n",
      "üìä RMSE trung b√¨nh qua 3 folds: 0.0728\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_lstm_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nüìÇ Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"‚úÖ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\nüìä RMSE trung b√¨nh qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d016bca",
   "metadata": {},
   "source": [
    "## Model evaluation and bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\n",
      "üìå MSE     : 0.0086 ¬± 0.0029\n",
      "üìå RMSE    : 0.0916 ¬± 0.0156\n",
      "üìå MAE     : 0.0653 ¬± 0.0107\n",
      "üìå R¬≤      : 0.9937 ¬± 0.0021\n",
      "üìå MAPE    : 32.12% ¬± 23.39%\n",
      "üìå MMRE    : 0.3212 ¬± 0.2339\n",
      "üìå MdMRE   : 0.0394 ¬± 0.0165\n",
      "üìå PRED(25): 88.12% ¬± 7.85%\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'lstm_evaluation_results_scaled.csv'\n",
      "\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'lstm_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ƒê√°nh gi√° bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\")\n",
    "print(f\"üìå MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ¬± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"üìå RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ¬± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"üìå MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ¬± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"üìå R¬≤      : {np.mean(bootstrap_metrics['r2']):.4f} ¬± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"üìå MAPE    : {np.nanmean(bootstrap_metrics['mape']):.2f}% ¬± {np.nanstd(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"üìå MMRE    : {np.nanmean(bootstrap_metrics['mmre']):.4f} ¬± {np.nanstd(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"üìå MdMRE   : {np.nanmean(bootstrap_metrics['mdmre']):.4f} ¬± {np.nanstd(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"üìå PRED(25): {np.nanmean(bootstrap_metrics['pred25']):.2f}% ¬± {np.nanstd(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.nanmean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.nanstd(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.nanmean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.nanstd(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.nanmean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.nanstd(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.nanmean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.nanstd(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('lstm_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'lstm_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung b√¨nh qua c√°c folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'lstm_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72345498",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd7a9d",
   "metadata": {},
   "source": [
    "## Import libraries and set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6017ea6",
   "metadata": {},
   "source": [
    "## Define the evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2443a",
   "metadata": {},
   "source": [
    "## Define custom Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6428123",
   "metadata": {},
   "source": [
    "MLP only enhances the data with Gaussian noise without reshaping. This means that the data stays in 2D form (samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21060513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.means_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numeric_cols:\n",
    "            self.means_[col] = X[col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col].fillna(self.means_[col], inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ ti√™u chu·∫©n h√≥a (ch·ªâ c√°c c·ªôt s·ªë)\n",
    "class SelectiveScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler_ = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.numeric_cols] = self.scaler_.transform(X_copy[self.numeric_cols])\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ tƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "class DataAugmenter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, noise_factor=0.01, n_copies=2):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.n_copies = n_copies\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_augmented = X.copy()\n",
    "        y_augmented = y.copy() if y is not None else None\n",
    "        for _ in range(self.n_copies):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_factor, size=X.shape)\n",
    "            X_noisy = X + noise\n",
    "            X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "            if y is not None:\n",
    "                y_augmented = np.hstack((y_augmented, y))\n",
    "        return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea75f2b",
   "metadata": {},
   "source": [
    "## Read and check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6cf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "features = numeric_columns + binary_columns\n",
    "target = 'Effort'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d74f93",
   "metadata": {},
   "source": [
    "## Create and apply a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a3d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation with Gaussian noise ===\n",
      "X_augmented shape: (243, 19)\n",
      "y_augmented shape: (243,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lequa\\AppData\\Local\\Temp\\ipykernel_11776\\1988351987.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(self.means_[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('data_type_converter', DataTypeConverter(numeric_cols=numeric_columns, binary_cols=binary_columns)),\n",
    "    ('nan_imputer', NaNImputer(numeric_cols=numeric_columns)),\n",
    "    ('scaler', SelectiveScaler(numeric_cols=numeric_columns)),\n",
    "])\n",
    "\n",
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "data_augmenter = DataAugmenter(noise_factor=0.01, n_copies=2)\n",
    "X_augmented, y_augmented = data_augmenter.transform(X_transformed.values, y)\n",
    "\n",
    "print(\"\\n=== After data augmentation with Gaussian noise ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fccbf37",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data size of MLP:\n",
      " - X_train: (206, 19)\n",
      " - X_test : (37, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n‚úÖ Data size of MLP:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618591c8",
   "metadata": {},
   "source": [
    "## Building MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f27aa",
   "metadata": {},
   "source": [
    "Define the build_mlp_model function to create an MLP model with the following architecture:\n",
    "\n",
    "- Input layer with shape (features,), i.e. 2D data.\n",
    "- First Dense layer with dense_units_1 unit and relu activation.\n",
    "- Dropout layer to reduce overfitting.\n",
    "- Second Dense layer with dense_units_2 units and relu activation.\n",
    "- Second Dropout layer.\n",
    "-  Dense layer with 1 unit and linear activation to predict Effort.\n",
    "\n",
    "Use Huber loss function and optimize with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(dense_units_1=64, dense_units_2=32, dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(dense_units_1, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units_2, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d2fbc",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d42986",
   "metadata": {},
   "source": [
    "Define hyperparameter space (param_bounds) for dense_units_1, dense_units_2, dropout_rate, learning_rate, batch_size, epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'dense_units_1': (32, 128),\n",
    "    'dense_units_2': (16, 64),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['dense_units_1'][0], param_bounds['dense_units_1'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units_2'][0], param_bounds['dense_units_2'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'dense_units_1': int(particle[0]),\n",
    "        'dense_units_2': int(particle[1]),\n",
    "        'dropout_rate': particle[2],\n",
    "        'learning_rate': particle[3],\n",
    "        'batch_size': int(particle[4]),\n",
    "        'epochs': int(particle[5])\n",
    "    }\n",
    "    params['dropout_rate'] = np.clip(params['dropout_rate'], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_mlp_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ecbb9",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1050\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0999\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0952\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0921\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0832\n",
      "\n",
      "üîÅ Iteration 3/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0772\n",
      "\n",
      "üîÅ Iteration 4/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0765\n",
      "\n",
      "üîÅ Iteration 5/10\n",
      "\n",
      "üîÅ Iteration 6/10\n",
      "\n",
      "üîÅ Iteration 7/10\n",
      "\n",
      "üîÅ Iteration 8/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0761\n",
      "\n",
      "üîÅ Iteration 9/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0724\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.0704\n",
      "\n",
      "üîÅ Iteration 10/10\n",
      "üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {'dense_units_1': 132, 'dense_units_2': 71, 'dropout_rate': np.float64(0.2), 'learning_rate': np.float64(0.0072998326507956734), 'batch_size': 28, 'epochs': 100}\n",
      "üìâ Score t·ªët nh·∫•t: 0.0704\n"
     ]
    }
   ],
   "source": [
    "def run_pso_mlp(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = np.clip(particles[i][2], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Ch·∫°y PSO\n",
    "print(\"üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\")\n",
    "best_particle, best_score = run_pso_mlp(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d722f",
   "metadata": {},
   "source": [
    "## Optimal model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.0870\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.0782\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.0647\n",
      "\n",
      "üìä RMSE trung b√¨nh qua 3 folds: 0.0766\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_mlp_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nüìÇ Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"‚úÖ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\nüìä RMSE trung b√¨nh qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c50b7",
   "metadata": {},
   "source": [
    "## Model evaluation and bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdaf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\n",
      "üìå MSE     : 0.0116 ¬± 0.0041\n",
      "üìå RMSE    : 0.1062 ¬± 0.0190\n",
      "üìå MAE     : 0.0778 ¬± 0.0117\n",
      "üìå R¬≤      : 0.9915 ¬± 0.0027\n",
      "üìå MAPE    : 82.88% ¬± 67.53%\n",
      "üìå MMRE    : 0.8288 ¬± 0.6753\n",
      "üìå MdMRE   : 0.0710 ¬± 0.0376\n",
      "üìå PRED(25): 82.21% ¬± 9.36%\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'mlp_evaluation_results_scaled.csv'\n",
      "\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'mlp_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ƒê√°nh gi√° bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\")\n",
    "print(f\"üìå MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ¬± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"üìå RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ¬± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"üìå MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ¬± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"üìå R¬≤      : {np.mean(bootstrap_metrics['r2']):.4f} ¬± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"üìå MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% ¬± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"üìå MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} ¬± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"üìå MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} ¬± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"üìå PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% ¬± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('mlp_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'mlp_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung b√¨nh qua c√°c folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mlp_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'mlp_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96eff4a",
   "metadata": {},
   "source": [
    "# RBFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed ƒë·ªÉ t√°i l·∫≠p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf086644",
   "metadata": {},
   "source": [
    "## Define the evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604879de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    return np.mean(mre <= 0.25) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917ad2c",
   "metadata": {},
   "source": [
    "## Define custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3b092",
   "metadata": {},
   "source": [
    "RBFN only enhances the data with Gaussian noise without reshaping. This means that the data is kept in 2D form (samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d134b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols, binary_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.binary_cols = binary_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce')\n",
    "        for col in self.binary_cols:\n",
    "            X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce').astype('int')\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ x·ª≠ l√Ω NaN\n",
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.means_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numeric_cols:\n",
    "            self.means_[col] = X[col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.numeric_cols:\n",
    "            X_copy[col].fillna(self.means_[col], inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ ti√™u chu·∫©n h√≥a (ch·ªâ c√°c c·ªôt s·ªë)\n",
    "class SelectiveScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler_ = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.numeric_cols] = self.scaler_.transform(X_copy[self.numeric_cols])\n",
    "        return X_copy\n",
    "\n",
    "# Transformer t√πy ch·ªânh ƒë·ªÉ tƒÉng c∆∞·ªùng d·ªØ li·ªáu\n",
    "class DataAugmenter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, noise_factor=0.01, n_copies=2):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.n_copies = n_copies\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_augmented = X.copy()\n",
    "        y_augmented = y.copy() if y is not None else None\n",
    "        for _ in range(self.n_copies):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_factor, size=X.shape)\n",
    "            X_noisy = X + noise\n",
    "            X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "            if y is not None:\n",
    "                y_augmented = np.hstack((y_augmented, y))\n",
    "        return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be274aec",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab20b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desharnais1.1_processed_corrected.csv')\n",
    "\n",
    "numeric_columns = ['TeamExp', 'ManagerExp', 'YearEnd', 'Length', \n",
    "                   'Transactions', 'Entities', 'Adjustment', 'PointsAjust',\n",
    "                   'StartYear', 'ProjectDurationYears', 'Transactions_Entities',\n",
    "                   'Effort_PointsAjust', 'Effort_per_PointsAjust', 'Transactions_per_Entities']\n",
    "binary_columns = ['Language_b\\'1\\'', 'Language_b\\'2\\'', 'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust']\n",
    "features = numeric_columns + binary_columns\n",
    "target = 'Effort'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36c512",
   "metadata": {},
   "source": [
    "## Create and apply a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0896a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After data augmentation ===\n",
      "X_augmented shape: (243, 19)\n",
      "y_augmented shape: (243,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lequa\\AppData\\Local\\Temp\\ipykernel_5832\\3040840636.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(self.means_[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('data_type_converter', DataTypeConverter(numeric_cols=numeric_columns, binary_cols=binary_columns)),\n",
    "    ('nan_imputer', NaNImputer(numeric_cols=numeric_columns)),\n",
    "    ('scaler', SelectiveScaler(numeric_cols=numeric_columns)),\n",
    "])\n",
    "\n",
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "data_augmenter = DataAugmenter(noise_factor=0.01, n_copies=2)\n",
    "X_augmented, y_augmented = data_augmenter.transform(X_transformed.values, y)\n",
    "\n",
    "print(\"\\n=== After data augmentation ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc695ed",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d018e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ K√≠ch th∆∞·ªõc d·ªØ li·ªáu RBFN:\n",
      " - X_train: (206, 19)\n",
      " - X_test : (37, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\n‚úÖ K√≠ch th∆∞·ªõc d·ªØ li·ªáu RBFN:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed48b5",
   "metadata": {},
   "source": [
    "## Building RBFN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dff73",
   "metadata": {},
   "source": [
    "Define the build_rbfn_model function to create an RBFN model with the following architecture:\n",
    "\n",
    "- Input layer with shape (features,), i.e. 2D data.\n",
    "- Custom RBFLayer layer with n_centers centers and sigma parameter, implementing radial (Gaussian) basis function.\n",
    "- Final Dense layer with 1 unit and linear activation to predict Effort.\n",
    "\n",
    "Using Huber loss function and optimizing with Adam.\n",
    "\n",
    "\n",
    "RBFN: Uses the RBFLayer class to implement radial (Gaussian) basis functions, based on the Euclidean distance between the input and the centers. This class is specific to RBFN and does not appear in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3cce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFLayer(Layer):\n",
    "    def __init__(self, n_centers, sigma=1.0, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.n_centers = n_centers\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                      shape=(self.n_centers, input_shape[1]),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        self.sigma = self.add_weight(name='sigma',\n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.sigma),\n",
    "                                    trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # T√≠nh kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng\n",
    "        diff = tf.expand_dims(inputs, axis=1) - tf.expand_dims(self.centers, axis=0)\n",
    "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "        # H√†m Gaussian\n",
    "        return tf.exp(-l2 / (2.0 * tf.square(self.sigma)))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_centers)\n",
    "\n",
    "def build_rbfn_model(n_centers=20, sigma=1.0, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        RBFLayer(n_centers, sigma=sigma),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a1402",
   "metadata": {},
   "source": [
    "## Definition of hyperparameter space and PSO function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb8674",
   "metadata": {},
   "source": [
    "Use n_centers (number of centers in RBFLayer) and sigma (parameter that adjusts the width of the Gaussian function), which are RBFN-specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "750597d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'n_centers': (10, 50),\n",
    "    'sigma': (0.1, 2.0),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# H√†m m√£ h√≥a & gi·∫£i m√£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['n_centers'][0], param_bounds['n_centers'][1] + 1),\n",
    "        np.random.uniform(param_bounds['sigma'][0], param_bounds['sigma'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'n_centers': int(particle[0]),\n",
    "        'sigma': particle[1],\n",
    "        'learning_rate': particle[2],\n",
    "        'batch_size': int(particle[3]),\n",
    "        'epochs': int(particle[4])\n",
    "    }\n",
    "    params['sigma'] = np.clip(params['sigma'], param_bounds['sigma'][0], param_bounds['sigma'][1])\n",
    "    return params\n",
    "\n",
    "# H√†m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rbfn_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9962273",
   "metadata": {},
   "source": [
    "## Implementing the PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc920c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\n",
      "\n",
      "üîÅ Iteration 1/10\n",
      "\n",
      "üîÅ Iteration 2/10\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1346\n",
      "‚úÖ C·∫≠p nh·∫≠t g_best: Score = 0.1313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Ch·∫°y PSO\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m best_particle, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pso_rbfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m best_params \u001b[38;5;241m=\u001b[39m decode_particle(best_particle)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müèÜ Si√™u tham s·ªë t·ªët nh·∫•t: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mrun_pso_rbfn\u001b[1;34m(num_particles, max_iter)\u001b[0m\n\u001b[0;32m     30\u001b[0m particles[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i], bounds_array[:, \u001b[38;5;241m0\u001b[39m], bounds_array[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     31\u001b[0m particles[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i][\u001b[38;5;241m1\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], param_bounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m p_best_scores[i]:\n\u001b[0;32m     36\u001b[0m     p_best_scores[i] \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(particle)\u001b[0m\n\u001b[0;32m     42\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    367\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:112\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lequa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_pso_rbfn(num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nüîÅ Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = np.clip(particles[i][1], param_bounds['sigma'][0], param_bounds['sigma'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"‚úÖ C·∫≠p nh·∫≠t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Ch·∫°y PSO\n",
    "print(\"üöÄ Ch·∫°y PSO ƒë·ªÉ t√¨m si√™u tham s·ªë t·ªëi ∆∞u...\")\n",
    "best_particle, best_score = run_pso_rbfn(num_particles=15, max_iter=10)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"üèÜ Si√™u tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
    "print(f\"üìâ Score t·ªët nh·∫•t: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d4166",
   "metadata": {},
   "source": [
    "## Optimal model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf6051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1/3\n",
      "‚úÖ Fold 1 RMSE: 0.1113\n",
      "\n",
      "üìÇ Fold 2/3\n",
      "‚úÖ Fold 2 RMSE: 0.1488\n",
      "\n",
      "üìÇ Fold 3/3\n",
      "‚úÖ Fold 3 RMSE: 0.0943\n",
      "\n",
      "üìä RMSE trung b√¨nh qua 3 folds: 0.1181\n"
     ]
    }
   ],
   "source": [
    "model_optimal = build_rbfn_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history_all = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nüìÇ Fold {fold + 1}/3\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"‚úÖ Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "    history_all['loss'].append(history.history['loss'])\n",
    "    history_all['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\nüìä RMSE trung b√¨nh qua 3 folds: {np.mean(rmse_scores_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714002a3",
   "metadata": {},
   "source": [
    "## Model evaluation and bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d5e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\n",
      "üìå MSE     : 0.0116 ¬± 0.0040\n",
      "üìå RMSE    : 0.1060 ¬± 0.0187\n",
      "üìå MAE     : 0.0777 ¬± 0.0118\n",
      "üìå R¬≤      : 0.9916 ¬± 0.0026\n",
      "üìå MAPE    : 82.30% ¬± 67.88%\n",
      "üìå MMRE    : 0.8230 ¬± 0.6788\n",
      "üìå MdMRE   : 0.0715 ¬± 0.0363\n",
      "üìå PRED(25): 82.35% ¬± 9.36%\n",
      "\n",
      "ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'rbfn_evaluation_results_scaled.csv'\n",
      "\n",
      "ƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'rbfn_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(h) for h in history_all['loss'])\n",
    "loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ƒê√°nh gi√° bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"\\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° bootstrap (tr√™n gi√° tr·ªã ƒë√£ scale):\")\n",
    "print(f\"üìå MSE     : {np.mean(bootstrap_metrics['mse']):.4f} ¬± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"üìå RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} ¬± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"üìå MAE     : {np.mean(bootstrap_metrics['mae']):.4f} ¬± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"üìå R¬≤      : {np.mean(bootstrap_metrics['r2']):.4f} ¬± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"üìå MAPE    : {np.nanmean(bootstrap_metrics['mape']):.2f}% ¬± {np.nanstd(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"üìå MMRE    : {np.nanmean(bootstrap_metrics['mmre']):.4f} ¬± {np.nanstd(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"üìå MdMRE   : {np.nanmean(bootstrap_metrics['mdmre']):.4f} ¬± {np.nanstd(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"üìå PRED(25): {np.nanmean(bootstrap_metrics['pred25']):.2f}% ¬± {np.nanstd(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.nanmean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.nanstd(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.nanmean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.nanstd(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.nanmean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.nanstd(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.nanmean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.nanstd(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('rbfn_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o 'rbfn_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss trung b√¨nh qua c√°c folds\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(loss_avg, label='Training Loss')\n",
    "plt.plot(val_loss_avg, label='Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rbfn_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nƒê√£ l∆∞u h√¨nh ·∫£nh tr·ª±c quan h√≥a v√†o 'rbfn_visualization_results_scaled.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
