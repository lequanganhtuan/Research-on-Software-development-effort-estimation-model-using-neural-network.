{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22011d63",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ede778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Kiá»ƒm tra dá»¯ liá»‡u ===\n",
      "KÃ­ch thÆ°á»›c: (24, 7)\n",
      "CÃ¡c cá»™t: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'Effort']\n",
      "Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\n",
      "      Input    Output   Inquiry      File     FPAdj  RawFPcounts    Effort\n",
      "0 -0.544978  2.676655  2.425098  2.235928  0.078752     2.030029  1.905014\n",
      "1  2.060336  1.583672  2.425098  1.423982  0.078752     2.030029  1.905014\n",
      "2  1.835512 -0.602296 -1.141891 -0.347538 -1.433293    -0.158757 -0.364603\n",
      "3  0.248519  0.413717  0.417448 -0.347538  1.212786     0.236318  0.476385\n",
      "4 -1.338474  0.690812 -1.063924 -0.568978 -0.677270    -0.336098  1.123946\n",
      "\n",
      "ThÃ´ng tin dá»¯ liá»‡u:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Input        24 non-null     float64\n",
      " 1   Output       24 non-null     float64\n",
      " 2   Inquiry      24 non-null     float64\n",
      " 3   File         24 non-null     float64\n",
      " 4   FPAdj        24 non-null     float64\n",
      " 5   RawFPcounts  24 non-null     float64\n",
      " 6   Effort       24 non-null     float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.4 KB\n",
      "None\n",
      "\n",
      "=== Sau khi tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian ===\n",
      "X_augmented shape: (72, 6)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau reshape ===\n",
      "X_augmented shape: (72, 6, 1)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "âœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u CNN:\n",
      " - X_train: (61, 6, 1)\n",
      " - X_test : (11, 6, 1)\n",
      "ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002BD480A5C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002BD480A5C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "ğŸ” Iteration 1/15\n",
      "\n",
      "ğŸ” Iteration 2/15\n",
      "\n",
      "ğŸ” Iteration 3/15\n",
      "\n",
      "ğŸ” Iteration 4/15\n",
      "\n",
      "ğŸ” Iteration 5/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3496\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3274\n",
      "\n",
      "ğŸ” Iteration 6/15\n",
      "\n",
      "ğŸ” Iteration 7/15\n",
      "\n",
      "ğŸ” Iteration 8/15\n",
      "\n",
      "ğŸ” Iteration 9/15\n",
      "\n",
      "ğŸ” Iteration 10/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3122\n",
      "\n",
      "ğŸ” Iteration 11/15\n",
      "\n",
      "ğŸ” Iteration 12/15\n",
      "\n",
      "ğŸ” Iteration 13/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3029\n",
      "\n",
      "ğŸ” Iteration 14/15\n",
      "\n",
      "ğŸ” Iteration 15/15\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'filters': 2, 'kernel_size': 2, 'l2_reg': 0.001, 'dense_units': 35, 'dropout_rate': np.float64(0.2072292288550723), 'learning_rate': np.float64(0.0009095629001577539), 'batch_size': 7, 'epochs': 107}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.3029\n",
      "\n",
      "ğŸ“‚ Fold 1/5\n",
      "âœ… Fold 1 RMSE: 0.6887\n",
      "\n",
      "ğŸ“‚ Fold 2/5\n",
      "âœ… Fold 2 RMSE: 0.5578\n",
      "\n",
      "ğŸ“‚ Fold 3/5\n",
      "âœ… Fold 3 RMSE: 0.2759\n",
      "\n",
      "ğŸ“‚ Fold 4/5\n",
      "âœ… Fold 4 RMSE: 0.2812\n",
      "\n",
      "ğŸ“‚ Fold 5/5\n",
      "âœ… Fold 5 RMSE: 0.2496\n",
      "\n",
      "ğŸ“Š RMSE trung bÃ¬nh qua 5 folds: 0.4106\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale):\n",
      "ğŸ“Œ MSE     : 0.2511 Â± 0.1338\n",
      "ğŸ“Œ RMSE    : 0.4805 Â± 0.1422\n",
      "ğŸ“Œ MAE     : 0.3276 Â± 0.1120\n",
      "ğŸ“Œ RÂ²      : 0.7246 Â± 0.1731\n",
      "ğŸ“Œ MAPE    : 37.11% Â± 11.45%\n",
      "ğŸ“Œ MMRE    : 0.3711 Â± 0.1145\n",
      "ğŸ“Œ MdMRE   : 0.2315 Â± 0.1709\n",
      "ğŸ“Œ PRED(25): 62.98% Â± 14.22%\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'cnn_evaluation_results_scaled.csv'\n",
      "\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'cnn_visualization_results_scaled.png'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from uuid import uuid4\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "    return np.mean(mre <= 0.25) * 100\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½\n",
    "df = pd.read_csv('albrecht_cleaned.csv')\n",
    "\n",
    "# Kiá»ƒm tra dá»¯ liá»‡u\n",
    "print(\"=== Kiá»ƒm tra dá»¯ liá»‡u ===\")\n",
    "print(\"KÃ­ch thÆ°á»›c:\", df.shape)\n",
    "print(\"CÃ¡c cá»™t:\", df.columns.tolist())\n",
    "print(\"Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\")\n",
    "print(df.head())\n",
    "print(\"\\nThÃ´ng tin dá»¯ liá»‡u:\")\n",
    "print(df.info())\n",
    "\n",
    "# Chá»n Ä‘áº·c trÆ°ng vÃ  biáº¿n má»¥c tiÃªu\n",
    "features = [col for col in df.columns if col not in ['Effort']]\n",
    "X = df[features].values\n",
    "y = df['Effort'].values  # Sá»­ dá»¥ng Effort Ä‘Ã£ Ä‘Æ°á»£c scale tá»« file test.csv\n",
    "\n",
    "# TÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def add_gaussian_noise(X, noise_factor=0.05):\n",
    "    noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "    return X + noise\n",
    "\n",
    "X_augmented = X.copy()\n",
    "y_augmented = y.copy()\n",
    "for _ in range(2):  # Táº¡o thÃªm 2 báº£n sao vá»›i nhiá»…u\n",
    "    X_noisy = add_gaussian_noise(X, noise_factor=0.05)\n",
    "    X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "    y_augmented = np.hstack((y_augmented, y))\n",
    "\n",
    "print(\"\\n=== Sau khi tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# Reshape dá»¯ liá»‡u thÃ nh dáº¡ng (samples, features, 1) cho Conv1D\n",
    "X_augmented = X_augmented.reshape(X_augmented.shape[0], X_augmented.shape[1], 1)\n",
    "\n",
    "print(\"\\n=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau reshape ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# Chia táº­p train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\nâœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u CNN:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")\n",
    "\n",
    "# XÃ¢y dá»±ng mÃ´ hÃ¬nh CNN vá»›i Conv1D, BatchNormalization, GlobalAveragePooling1D\n",
    "def build_cnn_model(filters=8, kernel_size=2, l2_reg=0.01, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Conv1D(filters, kernel_size, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters, kernel_size, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘\n",
    "param_bounds = {\n",
    "    'filters': (4, 32),\n",
    "    'kernel_size': (1, 3),\n",
    "    'l2_reg': (0.001, 0.1),\n",
    "    'dense_units': (8, 64),\n",
    "    'dropout_rate': (0.2, 0.5),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (8, 32),\n",
    "    'epochs': (50, 150)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['filters'][0], param_bounds['filters'][1] + 1),\n",
    "        np.random.randint(param_bounds['kernel_size'][0], param_bounds['kernel_size'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'filters': int(particle[0]),\n",
    "        'kernel_size': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dense_units': int(particle[3]),\n",
    "        'dropout_rate': particle[4],\n",
    "        'learning_rate': particle[5],\n",
    "        'batch_size': int(particle[6]),\n",
    "        'epochs': int(particle[7])\n",
    "    }\n",
    "    # Äáº£m báº£o l2_reg khÃ´ng Ã¢m\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    params['l2_reg'] = min(params['l2_reg'], param_bounds['l2_reg'][1])  # Giá»›i háº¡n trÃªn\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_cnn_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Triá»ƒn khai PSO\n",
    "def run_pso_cnn(num_particles=10, max_iter=15):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            # Äáº£m báº£o l2_reg khÃ´ng Ã¢m vÃ  trong giá»›i háº¡n\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            particles[i][2] = min(particles[i][2], param_bounds['l2_reg'][1])\n",
    "            particles[i][4] = np.clip(particles[i][4], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Cháº¡y PSO\n",
    "print(\"ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\")\n",
    "best_particle, best_score = run_pso_cnn(num_particles=10, max_iter=15)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "model_optimal = build_cnn_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nğŸ“‚ Fold {fold + 1}/5\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"âœ… Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RMSE trung bÃ¬nh qua 5 folds: {np.mean(rmse_scores_optimal):.4f}\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale):\")\n",
    "print(f\"ğŸ“Œ MSE     : {np.mean(bootstrap_metrics['mse']):.4f} Â± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"ğŸ“Œ RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} Â± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"ğŸ“Œ MAE     : {np.mean(bootstrap_metrics['mae']):.4f} Â± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"ğŸ“Œ RÂ²      : {np.mean(bootstrap_metrics['r2']):.4f} Â± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"ğŸ“Œ MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% Â± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"ğŸ“Œ MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} Â± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} Â± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% Â± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('cnn_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'cnn_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trá»±c quan hÃ³a káº¿t quáº£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'cnn_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d4d71",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Kiá»ƒm tra dá»¯ liá»‡u ===\n",
      "KÃ­ch thÆ°á»›c: (24, 7)\n",
      "CÃ¡c cá»™t: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'Effort']\n",
      "Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\n",
      "      Input    Output   Inquiry      File     FPAdj  RawFPcounts    Effort\n",
      "0 -0.544978  2.676655  2.425098  2.235928  0.078752     2.030029  1.905014\n",
      "1  2.060336  1.583672  2.425098  1.423982  0.078752     2.030029  1.905014\n",
      "2  1.835512 -0.602296 -1.141891 -0.347538 -1.433293    -0.158757 -0.364603\n",
      "3  0.248519  0.413717  0.417448 -0.347538  1.212786     0.236318  0.476385\n",
      "4 -1.338474  0.690812 -1.063924 -0.568978 -0.677270    -0.336098  1.123946\n",
      "\n",
      "ThÃ´ng tin dá»¯ liá»‡u:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Input        24 non-null     float64\n",
      " 1   Output       24 non-null     float64\n",
      " 2   Inquiry      24 non-null     float64\n",
      " 3   File         24 non-null     float64\n",
      " 4   FPAdj        24 non-null     float64\n",
      " 5   RawFPcounts  24 non-null     float64\n",
      " 6   Effort       24 non-null     float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.4 KB\n",
      "None\n",
      "\n",
      "=== Sau khi tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian ===\n",
      "X_augmented shape: (72, 6)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau tÄƒng cÆ°á»ng ===\n",
      "X_augmented shape: (72, 6)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "âœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u MLP:\n",
      " - X_train: (61, 6)\n",
      " - X_test : (11, 6)\n",
      "ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C9615CBCE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C9615CBCE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "ğŸ” Iteration 1/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3824\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3399\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3362\n",
      "\n",
      "ğŸ” Iteration 2/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3328\n",
      "\n",
      "ğŸ” Iteration 3/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2811\n",
      "\n",
      "ğŸ” Iteration 4/15\n",
      "\n",
      "ğŸ” Iteration 5/15\n",
      "\n",
      "ğŸ” Iteration 6/15\n",
      "\n",
      "ğŸ” Iteration 7/15\n",
      "\n",
      "ğŸ” Iteration 8/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2790\n",
      "\n",
      "ğŸ” Iteration 9/15\n",
      "\n",
      "ğŸ” Iteration 10/15\n",
      "\n",
      "ğŸ” Iteration 11/15\n",
      "\n",
      "ğŸ” Iteration 12/15\n",
      "\n",
      "ğŸ” Iteration 13/15\n",
      "\n",
      "ğŸ” Iteration 14/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2718\n",
      "\n",
      "ğŸ” Iteration 15/15\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'hidden_layers': 4, 'units_per_layer': 29, 'l2_reg': 0.1, 'dropout_rate': np.float64(0.1783543972687098), 'learning_rate': np.float64(0.0007720671085637227), 'batch_size': 3, 'epochs': 186}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.2718\n",
      "\n",
      "ğŸ“‚ Fold 1/5\n",
      "âœ… Fold 1 RMSE: 0.8441\n",
      "\n",
      "ğŸ“‚ Fold 2/5\n",
      "âœ… Fold 2 RMSE: 0.7190\n",
      "\n",
      "ğŸ“‚ Fold 3/5\n",
      "âœ… Fold 3 RMSE: 0.4486\n",
      "\n",
      "ğŸ“‚ Fold 4/5\n",
      "âœ… Fold 4 RMSE: 0.4940\n",
      "\n",
      "ğŸ“‚ Fold 5/5\n",
      "âœ… Fold 5 RMSE: 0.7034\n",
      "\n",
      "ğŸ“Š RMSE trung bÃ¬nh qua 5 folds: 0.6418\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale):\n",
      "ğŸ“Œ MSE     : 0.3518 Â± 0.1210\n",
      "ğŸ“Œ RMSE    : 0.5839 Â± 0.1042\n",
      "ğŸ“Œ MAE     : 0.4875 Â± 0.1032\n",
      "ğŸ“Œ RÂ²      : 0.5432 Â± 0.8945\n",
      "ğŸ“Œ MAPE    : 64.86% Â± 15.23%\n",
      "ğŸ“Œ MMRE    : 0.6486 Â± 0.1523\n",
      "ğŸ“Œ MdMRE   : 0.5745 Â± 0.2262\n",
      "ğŸ“Œ PRED(25): 27.49% Â± 13.68%\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'mlp_evaluation_results_scaled.csv'\n",
      "\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'mlp_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from uuid import uuid4\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "    return np.mean(mre <= 0.25) * 100\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½\n",
    "df = pd.read_csv('albrecht_cleaned.csv')\n",
    "\n",
    "# Kiá»ƒm tra dá»¯ liá»‡u\n",
    "print(\"=== Kiá»ƒm tra dá»¯ liá»‡u ===\")\n",
    "print(\"KÃ­ch thÆ°á»›c:\", df.shape)\n",
    "print(\"CÃ¡c cá»™t:\", df.columns.tolist())\n",
    "print(\"Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\")\n",
    "print(df.head())\n",
    "print(\"\\nThÃ´ng tin dá»¯ liá»‡u:\")\n",
    "print(df.info())\n",
    "\n",
    "# Chá»n Ä‘áº·c trÆ°ng vÃ  biáº¿n má»¥c tiÃªu\n",
    "features = [col for col in df.columns if col not in ['Project', 'Effort', 'Effort_log']]\n",
    "X = df[features].values\n",
    "y = df['Effort'].values  # Sá»­ dá»¥ng Effort Ä‘Ã£ Ä‘Æ°á»£c scale tá»« file test.csv\n",
    "\n",
    "# TÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def add_gaussian_noise(X, noise_factor=0.05):\n",
    "    noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "    return X + noise\n",
    "\n",
    "X_augmented = X.copy()\n",
    "y_augmented = y.copy()\n",
    "for _ in range(2):  # Táº¡o thÃªm 2 báº£n sao vá»›i nhiá»…u\n",
    "    X_noisy = add_gaussian_noise(X, noise_factor=0.05)\n",
    "    X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "    y_augmented = np.hstack((y_augmented, y))\n",
    "\n",
    "print(\"\\n=== Sau khi tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# KhÃ´ng cáº§n reshape cho MLP vÃ¬ MLP khÃ´ng yÃªu cáº§u dá»¯ liá»‡u 3D\n",
    "print(\"\\n=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau tÄƒng cÆ°á»ng ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# Chia táº­p train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\nâœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u MLP:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")\n",
    "\n",
    "# XÃ¢y dá»±ng mÃ´ hÃ¬nh MLP vá»›i Dense, BatchNormalization, Dropout\n",
    "def build_mlp_model(hidden_layers=2, units_per_layer=32, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    # Äáº£m báº£o l2_reg khÃ´ng Ã¢m\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    \n",
    "    # ThÃªm cÃ¡c táº§ng áº©n\n",
    "    for _ in range(int(hidden_layers)):\n",
    "        model.add(Dense(int(units_per_layer), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Táº§ng Ä‘áº§u ra\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho MLP\n",
    "param_bounds = {\n",
    "    'hidden_layers': (1, 4),  # Sá»‘ táº§ng áº©n\n",
    "    'units_per_layer': (16, 128),  # Sá»‘ Ä‘Æ¡n vá»‹ má»—i táº§ng\n",
    "    'l2_reg': (0.001, 0.1),\n",
    "    'dropout_rate': (0.2, 0.5),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 64),\n",
    "    'epochs': (50, 150)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['hidden_layers'][0], param_bounds['hidden_layers'][1] + 1),\n",
    "        np.random.randint(param_bounds['units_per_layer'][0], param_bounds['units_per_layer'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'hidden_layers': int(particle[0]),\n",
    "        'units_per_layer': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    # Äáº£m báº£o l2_reg khÃ´ng Ã¢m\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    params['l2_reg'] = min(params['l2_reg'], param_bounds['l2_reg'][1])\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_mlp_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Triá»ƒn khai PSO\n",
    "def run_pso_mlp(num_particles=10, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            # Äáº£m báº£o l2_reg khÃ´ng Ã¢m vÃ  trong giá»›i háº¡n\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            particles[i][2] = min(particles[i][2], param_bounds['l2_reg'][1])\n",
    "            particles[i][3] = np.clip(particles[i][3], param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Cháº¡y PSO\n",
    "print(\"ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\")\n",
    "best_particle, best_score = run_pso_mlp()\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "model_optimal = build_mlp_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nğŸ“‚ Fold {fold + 1}/5\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                               validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"âœ… Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RMSE trung bÃ¬nh qua 5 folds: {np.mean(rmse_scores_optimal):.4f}\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale):\")\n",
    "print(f\"ğŸ“Œ MSE     : {np.mean(bootstrap_metrics['mse']):.4f} Â± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"ğŸ“Œ RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} Â± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"ğŸ“Œ MAE     : {np.mean(bootstrap_metrics['mae']):.4f} Â± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"ğŸ“Œ RÂ²      : {np.mean(bootstrap_metrics['r2']):.4f} Â± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"ğŸ“Œ MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% Â± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"ğŸ“Œ MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} Â± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} Â± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% Â± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('mlp_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'mlp_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trá»±c quan hÃ³a káº¿t quáº£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mlp_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'mlp_visualization_results_scaled.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce20d50",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf29e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Kiá»ƒm tra dá»¯ liá»‡u ===\n",
      "KÃ­ch thÆ°á»›c ban Ä‘áº§u: (24, 7)\n",
      "CÃ¡c cá»™t: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'Effort']\n",
      "Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\n",
      "      Input    Output   Inquiry      File     FPAdj  RawFPcounts    Effort\n",
      "0 -0.544978  2.676655  2.425098  2.235928  0.078752     2.030029  1.905014\n",
      "1  2.060336  1.583672  2.425098  1.423982  0.078752     2.030029  1.905014\n",
      "2  1.835512 -0.602296 -1.141891 -0.347538 -1.433293    -0.158757 -0.364603\n",
      "3  0.248519  0.413717  0.417448 -0.347538  1.212786     0.236318  0.476385\n",
      "4 -1.338474  0.690812 -1.063924 -0.568978 -0.677270    -0.336098  1.123946\n",
      "\n",
      "=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau tÄƒng cÆ°á»ng vÃ  reshape ===\n",
      "X_augmented shape: (72, 1, 6)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "âœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u LSTM:\n",
      " - X_train: (57, 1, 6)\n",
      " - X_test : (15, 1, 6)\n",
      "ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\n",
      "\n",
      "ğŸ” Iteration 1/15\n",
      "âœ… Cáº­p nháº­t g_best: PRED(25) = 77.2727%\n",
      "âœ… Cáº­p nháº­t g_best: PRED(25) = 80.9091%\n",
      "\n",
      "ğŸ” Iteration 2/15\n",
      "âœ… Cáº­p nháº­t g_best: PRED(25) = 82.8788%\n",
      "\n",
      "ğŸ” Iteration 3/15\n",
      "âœ… Cáº­p nháº­t g_best: PRED(25) = 86.2121%\n",
      "\n",
      "ğŸ” Iteration 4/15\n",
      "\n",
      "ğŸ” Iteration 5/15\n",
      "\n",
      "ğŸ” Iteration 6/15\n",
      "\n",
      "ğŸ” Iteration 7/15\n",
      "\n",
      "ğŸ” Iteration 8/15\n",
      "\n",
      "ğŸ” Iteration 9/15\n",
      "\n",
      "ğŸ” Iteration 10/15\n",
      "\n",
      "ğŸ” Iteration 11/15\n",
      "\n",
      "ğŸ” Iteration 12/15\n",
      "\n",
      "ğŸ” Iteration 13/15\n",
      "\n",
      "ğŸ” Iteration 14/15\n",
      "\n",
      "ğŸ” Iteration 15/15\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'units': 59, 'dropout_rate': np.float64(0.250012773027689), 'learning_rate': np.float64(0.01175980209993104), 'batch_size': 2, 'epochs': 45}\n",
      "ğŸ“‰ PRED(25) tá»‘t nháº¥t: 86.2121%\n",
      "\n",
      "ğŸ“‚ Fold 1/5\n",
      "âœ… Fold 1 PRED(25): 41.6667%\n",
      "\n",
      "ğŸ“‚ Fold 2/5\n",
      "âœ… Fold 2 PRED(25): 41.6667%\n",
      "\n",
      "ğŸ“‚ Fold 3/5\n",
      "âœ… Fold 3 PRED(25): 54.5455%\n",
      "\n",
      "ğŸ“‚ Fold 4/5\n",
      "âœ… Fold 4 PRED(25): 63.6364%\n",
      "\n",
      "ğŸ“‚ Fold 5/5\n",
      "âœ… Fold 5 PRED(25): 54.5455%\n",
      "\n",
      "ğŸ“Š PRED(25) trung bÃ¬nh qua 5 folds: 51.2121%\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (thang gá»‘c):\n",
      "ğŸ“Œ MSE     : 0.0287 Â± 0.0103\n",
      "ğŸ“Œ RMSE    : 0.1667 Â± 0.0308\n",
      "ğŸ“Œ MAE     : 0.1339 Â± 0.0270\n",
      "ğŸ“Œ RÂ²      : 0.9621 Â± 0.0205\n",
      "ğŸ“Œ MAPE    : 24.42% Â± 6.42%\n",
      "ğŸ“Œ MMRE    : 0.2442 Â± 0.0642\n",
      "ğŸ“Œ MdMRE   : 0.1331 Â± 0.0566\n",
      "ğŸ“Œ PRED(25): 72.55% Â± 11.22%\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'lstm_evaluation_results.csv'\n",
      "\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'lstm_visualization_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "    return np.mean(mre <= 0.25) * 100\n",
    "\n",
    "# TÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def add_gaussian_noise(X, y, noise_factor=0.01):\n",
    "    X_noise = X + np.random.normal(loc=0, scale=noise_factor * np.std(X, axis=0), size=X.shape)\n",
    "    y_noise = y + np.random.normal(loc=0, scale=noise_factor * np.std(y), size=y.shape)\n",
    "    return X_noise, y_noise\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u\n",
    "df = pd.read_csv('albrecht_cleaned.csv')\n",
    "\n",
    "# Xá»­ lÃ½ ngoáº¡i lai vá»›i RobustScaler\n",
    "feature_scaler = RobustScaler()\n",
    "target_scaler = RobustScaler()\n",
    "features = [col for col in df.columns if col != 'Effort']\n",
    "X = feature_scaler.fit_transform(df[features].values)\n",
    "y = target_scaler.fit_transform(df[['Effort']].values).flatten()\n",
    "\n",
    "# TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "X_augmented = X.copy()\n",
    "y_augmented = y.copy()\n",
    "for _ in range(2):  # Táº¡o 2 báº£n sao vá»›i nhiá»…u, tÄƒng tá»« 24 lÃªn 72 máº«u\n",
    "    X_noise, y_noise = add_gaussian_noise(X, y, noise_factor=0.01)\n",
    "    X_augmented = np.vstack((X_augmented, X_noise))\n",
    "    y_augmented = np.hstack((y_augmented, y_noise))\n",
    "\n",
    "# Reshape dá»¯ liá»‡u cho LSTM: (samples, 1, features)\n",
    "X_augmented = X_augmented.reshape(X_augmented.shape[0], 1, X_augmented.shape[1])\n",
    "\n",
    "print(\"=== Kiá»ƒm tra dá»¯ liá»‡u ===\")\n",
    "print(\"KÃ­ch thÆ°á»›c ban Ä‘áº§u:\", df.shape)\n",
    "print(\"CÃ¡c cá»™t:\", df.columns.tolist())\n",
    "print(\"Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\")\n",
    "print(df.head())\n",
    "print(\"\\n=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau tÄƒng cÆ°á»ng vÃ  reshape ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# Chia táº­p train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nâœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u LSTM:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")\n",
    "\n",
    "# XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM Ä‘Æ¡n giáº£n\n",
    "def build_lstm_model(units=16, dropout_rate=0.1, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units // 2, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘\n",
    "param_bounds = {\n",
    "    'units': (16, 64),\n",
    "    'dropout_rate': (0.0, 0.2),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (4, 16),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['units'][0], param_bounds['units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    return {\n",
    "        'units': int(particle[0]),\n",
    "        'dropout_rate': particle[1],\n",
    "        'learning_rate': particle[2],\n",
    "        'batch_size': int(particle[3]),\n",
    "        'epochs': int(particle[4])\n",
    "    }\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_lstm_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pred25_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                  validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        pred25 = calculate_pred25(y_val, y_pred)\n",
    "        pred25_scores.append(pred25)\n",
    "    \n",
    "    return -np.mean(pred25_scores)  # Tá»‘i Æ°u hÃ³a PRED(25)\n",
    "\n",
    "# Triá»ƒn khai PSO\n",
    "def run_pso_lstm(num_particles=10, max_iter=15):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.7, 1.4, 1.4\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: PRED(25) = {-g_best_score:.4f}%\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Cháº¡y PSO\n",
    "print(\"ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\")\n",
    "best_particle, best_score = run_pso_lstm(num_particles=10, max_iter=15)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "print(f\"ğŸ“‰ PRED(25) tá»‘t nháº¥t: {-best_score:.4f}%\")\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "model_optimal = build_lstm_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "pred25_scores_optimal = []\n",
    "history = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nğŸ“‚ Fold {fold + 1}/5\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                               validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0).flatten()\n",
    "    pred25 = calculate_pred25(y_val, y_pred)\n",
    "    pred25_scores_optimal.append(pred25)\n",
    "    print(f\"âœ… Fold {fold + 1} PRED(25): {pred25:.4f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“Š PRED(25) trung bÃ¬nh qua 5 folds: {np.mean(pred25_scores_optimal):.4f}%\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Chuyá»ƒn ngÆ°á»£c vá» thang gá»‘c\n",
    "y_test_orig = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "y_pred_orig = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "mape = calculate_mape(y_test_orig, y_pred_orig)\n",
    "mmre = calculate_mmre(y_test_orig, y_pred_orig)\n",
    "mdmre = calculate_mdmre(y_test_orig, y_pred_orig)\n",
    "pred25 = calculate_pred25(y_test_orig, y_pred_orig)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test_orig), len(y_test_orig), replace=True)\n",
    "    y_test_boot = y_test_orig[indices]\n",
    "    y_pred_boot = y_pred_orig[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (thang gá»‘c):\")\n",
    "print(f\"ğŸ“Œ MSE     : {np.mean(bootstrap_metrics['mse']):.4f} Â± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"ğŸ“Œ RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} Â± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"ğŸ“Œ MAE     : {np.mean(bootstrap_metrics['mae']):.4f} Â± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"ğŸ“Œ RÂ²      : {np.mean(bootstrap_metrics['r2']):.4f} Â± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"ğŸ“Œ MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% Â± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"ğŸ“Œ MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} Â± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} Â± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% Â± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2 NRC': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('lstm_evaluation_results.csv', index=False)\n",
    "print(\"\\nÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'lstm_evaluation_results.csv'\")\n",
    "\n",
    "# Trá»±c quan hÃ³a káº¿t quáº£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test_orig, y_pred_orig, alpha=0.5)\n",
    "plt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Original Scale)')\n",
    "plt.xlabel('Actual Effort')\n",
    "plt.ylabel('Predicted Effort')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test_orig - y_pred_orig\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap PRED(25)\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=bootstrap_metrics['pred25'])\n",
    "plt.title('Bootstrap PRED(25) Distribution')\n",
    "plt.ylabel('PRED(25) (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_visualization_results.png')\n",
    "plt.close()\n",
    "print(\"\\nÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'lstm_visualization_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88f616",
   "metadata": {},
   "source": [
    "# RFBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e32517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Kiá»ƒm tra dá»¯ liá»‡u ===\n",
      "KÃ­ch thÆ°á»›c: (24, 7)\n",
      "CÃ¡c cá»™t: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'Effort']\n",
      "Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\n",
      "      Input    Output   Inquiry      File     FPAdj  RawFPcounts    Effort\n",
      "0 -0.544978  2.676655  2.425098  2.235928  0.078752     2.030029  1.905014\n",
      "1  2.060336  1.583672  2.425098  1.423982  0.078752     2.030029  1.905014\n",
      "2  1.835512 -0.602296 -1.141891 -0.347538 -1.433293    -0.158757 -0.364603\n",
      "3  0.248519  0.413717  0.417448 -0.347538  1.212786     0.236318  0.476385\n",
      "4 -1.338474  0.690812 -1.063924 -0.568978 -0.677270    -0.336098  1.123946\n",
      "\n",
      "ThÃ´ng tin dá»¯ liá»‡u:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Input        24 non-null     float64\n",
      " 1   Output       24 non-null     float64\n",
      " 2   Inquiry      24 non-null     float64\n",
      " 3   File         24 non-null     float64\n",
      " 4   FPAdj        24 non-null     float64\n",
      " 5   RawFPcounts  24 non-null     float64\n",
      " 6   Effort       24 non-null     float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.4 KB\n",
      "None\n",
      "\n",
      "=== Sau khi tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian ===\n",
      "X_augmented shape: (72, 6)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau tÄƒng cÆ°á»ng ===\n",
      "X_augmented shape: (72, 6)\n",
      "y_augmented shape: (72,)\n",
      "\n",
      "âœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u RBFN:\n",
      " - X_train: (61, 6)\n",
      " - X_test : (11, 6)\n",
      "ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\n",
      "\n",
      "ğŸ” Iteration 1/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3652\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3266\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2922\n",
      "\n",
      "ğŸ” Iteration 2/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2752\n",
      "\n",
      "ğŸ” Iteration 3/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2641\n",
      "\n",
      "ğŸ” Iteration 4/15\n",
      "\n",
      "ğŸ” Iteration 5/15\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2549\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2474\n",
      "\n",
      "ğŸ” Iteration 6/15\n",
      "\n",
      "ğŸ” Iteration 7/15\n",
      "\n",
      "ğŸ” Iteration 8/15\n",
      "\n",
      "ğŸ” Iteration 9/15\n",
      "\n",
      "ğŸ” Iteration 10/15\n",
      "\n",
      "ğŸ” Iteration 11/15\n",
      "\n",
      "ğŸ” Iteration 12/15\n",
      "\n",
      "ğŸ” Iteration 13/15\n",
      "\n",
      "ğŸ” Iteration 14/15\n",
      "\n",
      "ğŸ” Iteration 15/15\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'n_centers': 50, 'sigma': 0.1, 'l2_reg': 0.001, 'learning_rate': np.float64(0.008013404752108903), 'batch_size': 27, 'epochs': 151}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.2474\n",
      "\n",
      "ğŸ“‚ Fold 1/5\n",
      "âœ… Fold 1 RMSE: 0.8432\n",
      "\n",
      "ğŸ“‚ Fold 2/5\n",
      "âœ… Fold 2 RMSE: 0.3420\n",
      "\n",
      "ğŸ“‚ Fold 3/5\n",
      "âœ… Fold 3 RMSE: 0.1061\n",
      "\n",
      "ğŸ“‚ Fold 4/5\n",
      "âœ… Fold 4 RMSE: 0.0544\n",
      "\n",
      "ğŸ“‚ Fold 5/5\n",
      "âœ… Fold 5 RMSE: 0.0467\n",
      "\n",
      "ğŸ“Š RMSE trung bÃ¬nh qua 5 folds: 0.2785\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale):\n",
      "ğŸ“Œ MSE     : 0.2136 Â± 0.1224\n",
      "ğŸ“Œ RMSE    : 0.4397 Â± 0.1424\n",
      "ğŸ“Œ MAE     : 0.2877 Â± 0.1136\n",
      "ğŸ“Œ RÂ²      : 0.7637 Â± 0.1544\n",
      "ğŸ“Œ MAPE    : 29.84% Â± 9.51%\n",
      "ğŸ“Œ MMRE    : 0.2984 Â± 0.0951\n",
      "ğŸ“Œ MdMRE   : 0.2184 Â± 0.0954\n",
      "ğŸ“Œ PRED(25): 55.24% Â± 15.23%\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'rbfn_evaluation_results_scaled.csv'\n",
      "\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'rbfn_visualization_results_scaled.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.cluster import KMeans\n",
    "from uuid import uuid4\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_mmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def calculate_mdmre(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.median(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "def calculate_pred25(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mre = np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])\n",
    "    return np.mean(mre <= 0.25) * 100\n",
    "\n",
    "# Äá»‹nh nghÄ©a táº§ng RBF tÃ¹y chá»‰nh\n",
    "class RBFLayer(Layer):\n",
    "    def __init__(self, n_centers, centers=None, sigma=1.0, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.n_centers = n_centers\n",
    "        self.centers = centers  # Trung tÃ¢m Ä‘Æ°á»£c truyá»n tá»« KMeans\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.centers is None:\n",
    "            # Náº¿u khÃ´ng cÃ³ trung tÃ¢m, khá»Ÿi táº¡o ngáº«u nhiÃªn\n",
    "            self.centers = self.add_weight(name='centers',\n",
    "                                           shape=(self.n_centers, input_shape[-1]),\n",
    "                                           initializer='uniform',\n",
    "                                           trainable=False)\n",
    "        else:\n",
    "            # Sá»­ dá»¥ng trung tÃ¢m tá»« KMeans\n",
    "            self.centers = self.add_weight(name='centers',\n",
    "                                           shape=(self.n_centers, input_shape[-1]),\n",
    "                                           initializer=tf.keras.initializers.Constant(self.centers),\n",
    "                                           trainable=False)\n",
    "        self.sigma = self.add_weight(name='sigma',\n",
    "                                     shape=(),\n",
    "                                     initializer=tf.keras.initializers.Constant(self.sigma),\n",
    "                                     trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        diff = tf.expand_dims(inputs, axis=1) - self.centers  # (samples, n_centers, features)\n",
    "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)  # (samples, n_centers)\n",
    "        return tf.exp(-l2 / (2.0 * tf.square(self.sigma)))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_centers)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RBFLayer, self).get_config()\n",
    "        config.update({\n",
    "            'n_centers': self.n_centers,\n",
    "            'centers': self.centers.numpy() if isinstance(self.centers, tf.Variable) else self.centers,\n",
    "            'sigma': self.sigma.numpy() if isinstance(self.sigma, tf.Variable) else self.sigma\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½\n",
    "df = pd.read_csv('albrecht_cleaned.csv')\n",
    "\n",
    "# Kiá»ƒm tra dá»¯ liá»‡u\n",
    "print(\"=== Kiá»ƒm tra dá»¯ liá»‡u ===\")\n",
    "print(\"KÃ­ch thÆ°á»›c:\", df.shape)\n",
    "print(\"CÃ¡c cá»™t:\", df.columns.tolist())\n",
    "print(\"Máº«u 5 hÃ ng Ä‘áº§u tiÃªn:\")\n",
    "print(df.head())\n",
    "print(\"\\nThÃ´ng tin dá»¯ liá»‡u:\")\n",
    "print(df.info())\n",
    "\n",
    "# Chá»n Ä‘áº·c trÆ°ng vÃ  biáº¿n má»¥c tiÃªu\n",
    "features = [col for col in df.columns if col not in ['Project', 'Effort', 'Effort_log']]\n",
    "X = df[features].values\n",
    "y = df['Effort'].values  # Sá»­ dá»¥ng Effort Ä‘Ã£ Ä‘Æ°á»£c scale tá»« file test.csv\n",
    "\n",
    "# TÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def add_gaussian_noise(X, noise_factor=0.05):\n",
    "    noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "    return X + noise\n",
    "\n",
    "X_augmented = X.copy()\n",
    "y_augmented = y.copy()\n",
    "for _ in range(2):  # Táº¡o thÃªm 2 báº£n sao vá»›i nhiá»…u\n",
    "    X_noisy = add_gaussian_noise(X, noise_factor=0.05)\n",
    "    X_augmented = np.vstack((X_augmented, X_noisy))\n",
    "    y_augmented = np.hstack((y_augmented, y))\n",
    "\n",
    "print(\"\\n=== Sau khi tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# KhÃ´ng cáº§n reshape cho RBFN vÃ¬ dá»¯ liá»‡u lÃ  2D\n",
    "print(\"\\n=== KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau tÄƒng cÆ°á»ng ===\")\n",
    "print(\"X_augmented shape:\", X_augmented.shape)\n",
    "print(\"y_augmented shape:\", y_augmented.shape)\n",
    "\n",
    "# Chia táº­p train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"\\nâœ… KÃ­ch thÆ°á»›c dá»¯ liá»‡u RBFN:\")\n",
    "print(f\" - X_train: {X_train.shape}\")\n",
    "print(f\" - X_test : {X_test.shape}\")\n",
    "\n",
    "# XÃ¢y dá»±ng mÃ´ hÃ¬nh RBFN\n",
    "def build_rbfn_model(n_centers=10, sigma=1.0, l2_reg=0.01, learning_rate=0.001):\n",
    "    # Äáº£m báº£o l2_reg khÃ´ng Ã¢m vÃ  sigma dÆ°Æ¡ng\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    sigma = max(sigma, 0.1)\n",
    "    \n",
    "    # Sá»­ dá»¥ng KMeans Ä‘á»ƒ khá»Ÿi táº¡o centers\n",
    "    kmeans = KMeans(n_clusters=int(n_centers), random_state=42).fit(X_train)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        RBFLayer(n_centers, centers=centers, sigma=sigma),\n",
    "        Dense(1, activation='linear', kernel_regularizer=l2(l2_reg))\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho RBFN\n",
    "param_bounds = {\n",
    "    'n_centers': (5, 50),  # Sá»‘ trung tÃ¢m (nÆ¡-ron áº©n)\n",
    "    'sigma': (0.1, 5.0),  # Äá»™ rá»™ng cá»§a hÃ m Gaussian\n",
    "    'l2_reg': (0.001, 0.1),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 64),\n",
    "    'epochs': (50, 150)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['n_centers'][0], param_bounds['n_centers'][1] + 1),\n",
    "        np.random.uniform(param_bounds['sigma'][0], param_bounds['sigma'][1]),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'n_centers': int(particle[0]),\n",
    "        'sigma': particle[1],\n",
    "        'l2_reg': particle[2],\n",
    "        'learning_rate': particle[3],\n",
    "        'batch_size': int(particle[4]),\n",
    "        'epochs': int(particle[5])\n",
    "    }\n",
    "    # Äáº£m báº£o l2_reg khÃ´ng Ã¢m vÃ  sigma dÆ°Æ¡ng\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    params['l2_reg'] = min(params['l2_reg'], param_bounds['l2_reg'][1])\n",
    "    params['sigma'] = max(params['sigma'], 0.1)  # Sigma pháº£i dÆ°Æ¡ng\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rbfn_model(**{k: v for k, v in params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Triá»ƒn khai PSO\n",
    "def run_pso_rbfn(num_particles=10, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    \n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            # Äáº£m báº£o l2_reg khÃ´ng Ã¢m vÃ  sigma dÆ°Æ¡ng\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['sigma'][0])  # sigma\n",
    "            particles[i][1] = min(particles[i][1], param_bounds['sigma'][1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])  # l2_reg\n",
    "            particles[i][2] = min(particles[i][2], param_bounds['l2_reg'][1])\n",
    "            \n",
    "            score = fitness_function(particles[i])\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Cháº¡y PSO\n",
    "print(\"ğŸš€ Cháº¡y PSO Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u...\")\n",
    "best_particle, best_score = run_pso_rbfn(num_particles=10, max_iter=15)\n",
    "best_params = decode_particle(best_particle)\n",
    "print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "model_optimal = build_rbfn_model(**{k: v for k, v in best_params.items() if k != 'batch_size' and k != 'epochs'})\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores_optimal = []\n",
    "history = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\nğŸ“‚ Fold {fold + 1}/5\")\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model_optimal.fit(X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                               validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    y_pred = model_optimal.predict(X_val, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_scores_optimal.append(rmse)\n",
    "    print(f\"âœ… Fold {fold + 1} RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RMSE trung bÃ¬nh qua 5 folds: {np.mean(rmse_scores_optimal):.4f}\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "y_pred = model_optimal.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "mmre = calculate_mmre(y_test, y_pred)\n",
    "mdmre = calculate_mdmre(y_test, y_pred)\n",
    "pred25 = calculate_pred25(y_test, y_pred)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ bootstrap\n",
    "n_bootstraps = 500\n",
    "bootstrap_metrics = {'mse': [], 'mae': [], 'r2': [], 'mape': [], 'mmre': [], 'mdmre': [], 'pred25': []}\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_test_boot = y_test[indices]\n",
    "    y_pred_boot = y_pred[indices]\n",
    "    bootstrap_metrics['mse'].append(mean_squared_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mae'].append(mean_absolute_error(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['r2'].append(r2_score(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mape'].append(calculate_mape(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mmre'].append(calculate_mmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['mdmre'].append(calculate_mdmre(y_test_boot, y_pred_boot))\n",
    "    bootstrap_metrics['pred25'].append(calculate_pred25(y_test_boot, y_pred_boot))\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ bootstrap (trÃªn giÃ¡ trá»‹ Ä‘Ã£ scale):\")\n",
    "print(f\"ğŸ“Œ MSE     : {np.mean(bootstrap_metrics['mse']):.4f} Â± {np.std(bootstrap_metrics['mse']):.4f}\")\n",
    "print(f\"ğŸ“Œ RMSE    : {np.mean(np.sqrt(bootstrap_metrics['mse'])):.4f} Â± {np.std(np.sqrt(bootstrap_metrics['mse'])):.4f}\")\n",
    "print(f\"ğŸ“Œ MAE     : {np.mean(bootstrap_metrics['mae']):.4f} Â± {np.std(bootstrap_metrics['mae']):.4f}\")\n",
    "print(f\"ğŸ“Œ RÂ²      : {np.mean(bootstrap_metrics['r2']):.4f} Â± {np.std(bootstrap_metrics['r2']):.4f}\")\n",
    "print(f\"ğŸ“Œ MAPE    : {np.mean(bootstrap_metrics['mape']):.2f}% Â± {np.std(bootstrap_metrics['mape']):.2f}%\")\n",
    "print(f\"ğŸ“Œ MMRE    : {np.mean(bootstrap_metrics['mmre']):.4f} Â± {np.std(bootstrap_metrics['mmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ MdMRE   : {np.mean(bootstrap_metrics['mdmre']):.4f} Â± {np.std(bootstrap_metrics['mdmre']):.4f}\")\n",
    "print(f\"ğŸ“Œ PRED(25): {np.mean(bootstrap_metrics['pred25']):.2f}% Â± {np.std(bootstrap_metrics['pred25']):.2f}%\")\n",
    "\n",
    "# LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "results = {\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2,\n",
    "    'MAPE': mape,\n",
    "    'MMRE': mmre,\n",
    "    'MdMRE': mdmre,\n",
    "    'PRED(25)': pred25,\n",
    "    'Bootstrap_MSE_Mean': np.mean(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MSE_Std': np.std(bootstrap_metrics['mse']),\n",
    "    'Bootstrap_MAE_Mean': np.mean(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_MAE_Std': np.std(bootstrap_metrics['mae']),\n",
    "    'Bootstrap_R2_Mean': np.mean(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_R2_Std': np.std(bootstrap_metrics['r2']),\n",
    "    'Bootstrap_MAPE_Mean': np.mean(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MAPE_Std': np.std(bootstrap_metrics['mape']),\n",
    "    'Bootstrap_MMRE_Mean': np.mean(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MMRE_Std': np.std(bootstrap_metrics['mmre']),\n",
    "    'Bootstrap_MdMRE_Mean': np.mean(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_MdMRE_Std': np.std(bootstrap_metrics['mdmre']),\n",
    "    'Bootstrap_PRED25_Mean': np.mean(bootstrap_metrics['pred25']),\n",
    "    'Bootstrap_PRED25_Std': np.std(bootstrap_metrics['pred25'])\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('rbfn_evaluation_results_scaled.csv', index=False)\n",
    "print(\"\\nÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o 'rbfn_evaluation_results_scaled.csv'\")\n",
    "\n",
    "# Trá»±c quan hÃ³a káº¿t quáº£\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Predicted vs Actual Effort (Scaled)')\n",
    "plt.xlabel('Actual Effort (Scaled)')\n",
    "plt.ylabel('Predicted Effort (Scaled)')\n",
    "\n",
    "# Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error (Scaled)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Bootstrap RMSE\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(y=np.sqrt(bootstrap_metrics['mse']))\n",
    "plt.title('Bootstrap RMSE Distribution (Scaled)')\n",
    "plt.ylabel('RMSE (Scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rbfn_visualization_results_scaled.png')\n",
    "plt.close()\n",
    "print(\"\\nÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'rbfn_visualization_results_scaled.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
